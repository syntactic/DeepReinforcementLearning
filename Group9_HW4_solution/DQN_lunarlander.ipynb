{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKuKSCjZduHz",
        "outputId": "e9768846-d721-486d-d4de-ef82b782ddfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[accept-rom-license,atari,box2d]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d]) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d]) (4.1.1)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari,box2d])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari,box2d]) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,box2d]) (3.4)\n",
            "Building wheels for collected packages: box2d-py, AutoROM.accept-rom-license\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2811825 sha256=88e7f99052596019c61a3777bb4b2deeefb097966af7c724ce10ae8a9824fab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=f57e018b3aebad8bba02759a3275c530b4641acd7193afeac44f9d829f15c58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built box2d-py AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, box2d-py, pygame, jax-jumpy, ale-py, gymnasium, AutoROM.accept-rom-license, autorom, shimmy\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 pygame-2.1.3 shimmy-0.2.1\n"
          ]
        }
      ],
      "source": [
        "%pip install swig\n",
        "%pip install gymnasium[atari,accept-rom-license,box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8xqEfgaduH1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJyk6uWQduH1"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, max_size: int, environment_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps:int):\n",
        "        self.max_size = max_size\n",
        "        self.environment_name = environment_name\n",
        "        self.parallel_game_unrolls = parallel_game_unrolls\n",
        "        self.unroll_steps = unroll_steps # how many steps until game terminates (1000)\n",
        "        self.observation_preprocessing_function = observation_preprocessing_function\n",
        "        self.num_possible_actions = gym.make(environment_name).action_space.n\n",
        "        self.envs = gym.vector.make(environment_name, num_envs=self.parallel_game_unrolls) # num_envs is parallel games\n",
        "        self.current_states, _ = self.envs.reset()\n",
        "        self.data = []\n",
        "\n",
        "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "        # adds new samples into the ERP\n",
        "\n",
        "        # create empty containers to store the sample data\n",
        "        states_list = []\n",
        "        actions_list = []\n",
        "        rewards_list = []\n",
        "        terminateds_list = []\n",
        "        next_states_list = []\n",
        "\n",
        "        done = False\n",
        "        episodes_finished = np.zeros(self.parallel_game_unrolls, dtype=bool)\n",
        "        test_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            actions = self.sample_epsilon_greedy(dqn_network, epsilon) # (PARALLEL_GAME_UNROLLS, )\n",
        "            # take the action and get s' and r\n",
        "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "            # store observation, action, reward, next_observation into ERP container\n",
        "            #\n",
        "            states_list.append(self.current_states)\n",
        "            actions_list.append(actions)\n",
        "            rewards_list.append(rewards)\n",
        "            terminateds_list.append(terminateds)\n",
        "            next_states_list.append(next_states)\n",
        "            self.current_states = next_states\n",
        "\n",
        "            episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
        "\n",
        "            test_steps += 1\n",
        "            done = np.all(episodes_finished) or test_steps >= self.unroll_steps # exit loop if all games are finished or exeed max unroll steps\n",
        "\n",
        "\n",
        "        def data_generator():\n",
        "            for states_batch, actions_batch, rewards_batch, terminateds_batch, next_states_batch in \\\n",
        "                zip(states_list, actions_list, rewards_list, terminateds_list, next_states_list):\n",
        "                for game_idx in range(self.parallel_game_unrolls):\n",
        "                    state = states_batch[game_idx,:]\n",
        "                    action = actions_batch[game_idx]\n",
        "                    reward = rewards_batch[game_idx]\n",
        "                    terminated = terminateds_batch[game_idx]\n",
        "                    next_state = next_states_batch[game_idx, :]\n",
        "                    yield(state, action, reward, next_state, terminated)\n",
        "\n",
        "        dataset_tensor_specs = (tf.TensorSpec(shape=(8), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(8), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.bool))\n",
        "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "\n",
        "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, next_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(next_state), terminated))\n",
        "        new_samples_dataset = new_samples_dataset.cache().shuffle(buffer_size=self.unroll_steps * self.parallel_game_unrolls, reshuffle_each_iteration=True)\n",
        "\n",
        "        for elem in new_samples_dataset:\n",
        "            continue\n",
        "\n",
        "        self.data.append(new_samples_dataset)\n",
        "\n",
        "        datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps\n",
        "        if datapoints_in_data > self.max_size:\n",
        "            self.data.pop(0)\n",
        "\n",
        "    def create_dataset(self):\n",
        "        ERP_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False)\n",
        "        return ERP_dataset\n",
        "\n",
        "    def sample_epsilon_greedy(self, dqn_network, epsilon):\n",
        "        observations = self.observation_preprocessing_function(self.current_states)\n",
        "        q_values = dqn_network(observations) # tensor float 32 shape(parallel_game_unrolls, num_actions)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1)\n",
        "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy()\n",
        "        return actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyQb-yyNduH3"
      },
      "outputs": [],
      "source": [
        "def observation_preprocessing_function(observation):\n",
        "    return observation\n",
        "\n",
        "def create_dqn_model_lunar_lander(num_actions: int):\n",
        "    input_layer = tf.keras.Input(shape=(8), dtype=tf.float32)\n",
        "    x=tf.keras.layers.Dense(units=128, activation='relu')(input_layer)\n",
        "    x=tf.keras.layers.Dense(units=128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
        "    return model\n",
        "\n",
        "def train_dqn(train_dqn_network, target_network, dataset, optimizer, gamma: float, num_training_steps: int, batch_size: int=64):\n",
        "    dataset = dataset.batch(batch_size).prefetch(4)\n",
        "    @tf.function\n",
        "    def training_step(q_target, observations, actions):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_predictions_all_actions = train_dqn_network(observations) # shape of q_predictions is (batch_size, num_actions)\n",
        "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1)\n",
        "            loss = tf.reduce_mean(tf.square(q_predictions - q_target))\n",
        "        gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    losses = []\n",
        "    q_values = []\n",
        "    for i, state_transition in enumerate(dataset):\n",
        "        state, action, reward, subsequent_state, terminated = state_transition\n",
        "        # calculate q_target\n",
        "\n",
        "        train_q_vals = train_dqn_network(subsequent_state)\n",
        "        target_q_vals = target_network(subsequent_state)\n",
        "        train_q_argmax = tf.argmax(train_q_vals, axis=1)\n",
        "        target_q_values_argmax_train = tf.gather(target_q_vals, train_q_argmax, axis=1)\n",
        "        q_values.append(target_q_values_argmax_train.numpy())\n",
        "\n",
        "        use_subsequent_state = tf.where(terminated, tf.zeros_like(target_q_values_argmax_train, dtype=tf.float32), tf.ones_like(target_q_values_argmax_train, dtype=tf.float32))\n",
        "        q_target = reward + (gamma*target_q_values_argmax_train*use_subsequent_state)\n",
        "        loss = training_step(q_target, observations=state, actions=action)\n",
        "        losses.append(loss)\n",
        "        if i >= num_training_steps:\n",
        "            break\n",
        "    return np.mean(losses), np.mean(q_values)\n",
        "\n",
        "\n",
        "def test_q_network(test_dqn_network, environment_name: str, num_parallel_tests: int, gamma: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
        "    envs = gym.vector.make(environment_name, num_parallel_tests)\n",
        "    num_possible_actions = envs.single_action_space.n\n",
        "    states, _ = envs.reset()\n",
        "    done = False\n",
        "    timestep = 0\n",
        "    # episodes_finished is np vector of shape (num_parallel_tests,), filled with booleans, starting with all False\n",
        "    episodes_finished = np.zeros(num_parallel_tests, dtype=bool)\n",
        "    returns = np.zeros(num_parallel_tests)\n",
        "    test_steps = 0\n",
        "    while not done:\n",
        "        states = preprocessing_function(states)\n",
        "        q_values = test_dqn_network(states)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_tests,)\n",
        "        random_actions = tf.random.uniform(shape=(num_parallel_tests, ), minval=0,\n",
        "                                           maxval=num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_tests,), minval=0,\n",
        "                                             maxval=1, dtype=tf.float32) > test_epsilon # tensor of type tf.bool, shape (num_parallel_tests,)\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (num_parallel_tests,)\n",
        "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
        "        # compute pointwise or between episodes_finished and terminateds\n",
        "        episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
        "        returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32))\n",
        "        timestep += 1\n",
        "        # done if all episodes are finished\n",
        "        done = np.all(episodes_finished)\n",
        "        test_steps += 1\n",
        "        if test_steps % 100 == 0:\n",
        "           print(f\"test_steps: {test_steps} {np.sum(episodes_finished)/num_parallel_tests} {terminateds.shape} {episodes_finished.shape}\")\n",
        "    return np.mean(returns)\n",
        "\n",
        "def visualize_testing(results_df, name=\"\"):\n",
        "    # create three subplots\n",
        "    fig, axis = plt.subplots(1, 3)\n",
        "    # include the row idxs explicitly in the results_df\n",
        "    results_df['step'] = results_df.index\n",
        "    # plot the average return\n",
        "    sns.lineplot(x='step', y='average_return', data=results_df, ax=axis[0])\n",
        "    # plot the average loss\n",
        "    sns.lineplot(x='step', y='average_loss', data=results_df, ax=axis[1])\n",
        "    # plot the average q values\n",
        "    sns.lineplot(x='step', y='average_q_values', data=results_df, ax=axis[2])\n",
        "    # save the figure\n",
        "    # create a timestring from the timestamp\n",
        "    timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # save the figure\n",
        "    plt.savefig(f\"results\" + name + \".png\")\n",
        "    # close the figure\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def polyak_averaging_weights(source_network, target_network, polyak_averaging_factor: float):\n",
        "    source_network_weights = source_network.get_weights()\n",
        "    target_network_weights = target_network.get_weights()\n",
        "    averaged_weights = []\n",
        "    for source_weight, target_weight in zip(source_network_weights, target_network_weights):\n",
        "        fraction_kept_weights = polyak_averaging_factor * target_weight\n",
        "        fraction_updated_weights = (1-polyak_averaging_factor) * source_weight\n",
        "        averaged_weight = fraction_kept_weights + fraction_updated_weights\n",
        "        averaged_weights.append(averaged_weight)\n",
        "    target_network.set_weights(averaged_weights)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAfMX2BgduH4"
      },
      "outputs": [],
      "source": [
        "def dqn():\n",
        "    ENVIRONMENT_NAME = \"LunarLander-v2\"\n",
        "    NUMBER_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "    ERP_SIZE = 500000\n",
        "    PARALLEL_GAME_UNROLLS = 16 # 128\n",
        "    UNROLL_STEPS = 1000\n",
        "    EPSILON = 1\n",
        "    EPSILON_MIN = 0.01 # 0.01\n",
        "    EPSILON_DECAY = 0.995\n",
        "    LEARNING_RATE = 0.001\n",
        "    GAMMA = 0.99\n",
        "    NUM_TRAINING_STEPS_PER_ITER = 4 # 4\n",
        "    NUM_TRAINING_ITERS = 500\n",
        "    TEST_EVERY_N_STEPS = 10\n",
        "    TEST_NUM_PARALLEL_ENVS = 64\n",
        "    PREFILL_STEPS = 64 # 50\n",
        "    POLYAK_AVERAGING_FACTOR = 0.98 # 0.99\n",
        "    erp = ExperienceReplayBuffer(max_size=ERP_SIZE, environment_name=ENVIRONMENT_NAME,\n",
        "                                parallel_game_unrolls=PARALLEL_GAME_UNROLLS, unroll_steps=UNROLL_STEPS,\n",
        "                                observation_preprocessing_function=observation_preprocessing_function)\n",
        "\n",
        "    # This is the DQN we train\n",
        "    dqn_agent = create_dqn_model_lunar_lander(num_actions=NUMBER_ACTIONS)\n",
        "    # This is the target network, used to calculate the q-estimation targets\n",
        "    target_network = create_dqn_model_lunar_lander(num_actions=NUMBER_ACTIONS)\n",
        "    dqn_agent.summary()\n",
        "    # copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.0\n",
        "    polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=0.0)\n",
        "\n",
        "    dqn_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    return_tracker = []\n",
        "    dqn_prediction_error = []\n",
        "    average_q_values = []\n",
        "\n",
        "    # prefill the replay buffer\n",
        "    prefill_exploration_epsilon = 1.\n",
        "    for prefill_step in range(PREFILL_STEPS):\n",
        "        erp.fill_with_samples(dqn_agent, epsilon=prefill_exploration_epsilon)\n",
        "\n",
        "\n",
        "    for step in range(1,NUM_TRAINING_ITERS):\n",
        "        print(f'Training step: {step}')\n",
        "        # step 1: put some s, a, r, s', t transitions into the replay buffer\n",
        "        erp.fill_with_samples(dqn_agent, epsilon=max(EPSILON * EPSILON_DECAY**step, EPSILON_MIN))\n",
        "        dataset = erp.create_dataset()\n",
        "        # step 2: train on some samples from the replay buffer\n",
        "        average_loss, average_q_vals = train_dqn(dqn_agent, target_network, dataset, dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITER)\n",
        "\n",
        "        # update the target network via polyak averaging\n",
        "        polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "        if step % TEST_EVERY_N_STEPS == 0 or step == (NUM_TRAINING_ITERS - 1):\n",
        "            average_return = test_q_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA,\n",
        "                                            preprocessing_function=observation_preprocessing_function)\n",
        "            return_tracker.append(average_return)\n",
        "            dqn_prediction_error.append(average_loss)\n",
        "            average_q_values.append(average_q_vals)\n",
        "            # print average returns, average loss, average q values\n",
        "            print(f\"TESTING: Average return: {average_return}, Average loss: {average_loss}, Average q value-estimation: {average_q_vals}\")\n",
        "            # put all result lists into a dataframe by transforming them into a dict first\n",
        "            results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values}\n",
        "            results_df = pd.DataFrame(results_dict)\n",
        "            # visualize the results with sns\n",
        "            visualize_testing(results_df)\n",
        "            print(results_df)\n",
        "\n",
        "    # Save the weights\n",
        "    dqn_agent.save_weights('./dqn_agent.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNeMvrjFduH4",
        "outputId": "bbd8d4c3-46e2-4043-f226-2ff6fd58625c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 8)]               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               1152      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,180\n",
            "Trainable params: 18,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training step: 1\n",
            "Training step: 2\n",
            "Training step: 3\n",
            "Training step: 4\n",
            "Training step: 5\n",
            "Training step: 6\n",
            "Training step: 7\n",
            "Training step: 8\n",
            "Training step: 9\n",
            "Training step: 10\n",
            "test_steps: 100 0.484375 (64,) (64,)\n",
            "test_steps: 200 0.984375 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -335.4029206959142, Average loss: 119.78843688964844, Average q value-estimation: 0.5691779255867004\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "Training step: 11\n",
            "Training step: 12\n",
            "Training step: 13\n",
            "Training step: 14\n",
            "Training step: 15\n",
            "Training step: 16\n",
            "Training step: 17\n",
            "Training step: 18\n",
            "Training step: 19\n",
            "Training step: 20\n",
            "test_steps: 100 0.515625 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "TESTING: Average return: -306.2302456299907, Average loss: 114.5762939453125, Average q value-estimation: 1.083605408668518\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "Training step: 21\n",
            "Training step: 22\n",
            "Training step: 23\n",
            "Training step: 24\n",
            "Training step: 25\n",
            "Training step: 26\n",
            "Training step: 27\n",
            "Training step: 28\n",
            "Training step: 29\n",
            "Training step: 30\n",
            "test_steps: 100 0.546875 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -322.7926455825732, Average loss: 103.7659683227539, Average q value-estimation: 0.80859375\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "Training step: 31\n",
            "Training step: 32\n",
            "Training step: 33\n",
            "Training step: 34\n",
            "Training step: 35\n",
            "Training step: 36\n",
            "Training step: 37\n",
            "Training step: 38\n",
            "Training step: 39\n",
            "Training step: 40\n",
            "test_steps: 100 0.515625 (64,) (64,)\n",
            "test_steps: 200 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -302.53111877063407, Average loss: 79.68666076660156, Average q value-estimation: -0.4909185469150543\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "Training step: 41\n",
            "Training step: 42\n",
            "Training step: 43\n",
            "Training step: 44\n",
            "Training step: 45\n",
            "Training step: 46\n",
            "Training step: 47\n",
            "Training step: 48\n",
            "Training step: 49\n",
            "Training step: 50\n",
            "test_steps: 100 0.5 (64,) (64,)\n",
            "test_steps: 200 0.921875 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -273.1207347054559, Average loss: 120.68204498291016, Average q value-estimation: -0.06471947580575943\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "Training step: 51\n",
            "Training step: 52\n",
            "Training step: 53\n",
            "Training step: 54\n",
            "Training step: 55\n",
            "Training step: 56\n",
            "Training step: 57\n",
            "Training step: 58\n",
            "Training step: 59\n",
            "Training step: 60\n",
            "test_steps: 100 0.5 (64,) (64,)\n",
            "test_steps: 200 0.921875 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -306.00881323070706, Average loss: 104.09295654296875, Average q value-estimation: -0.4737062454223633\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "5     -306.008813    104.092957         -0.473706     5\n",
            "Training step: 61\n",
            "Training step: 62\n",
            "Training step: 63\n",
            "Training step: 64\n",
            "Training step: 65\n",
            "Training step: 66\n",
            "Training step: 67\n",
            "Training step: 68\n",
            "Training step: 69\n",
            "Training step: 70\n",
            "test_steps: 100 0.265625 (64,) (64,)\n",
            "test_steps: 200 0.765625 (64,) (64,)\n",
            "test_steps: 300 0.921875 (64,) (64,)\n",
            "TESTING: Average return: -169.0185299108193, Average loss: 102.4626235961914, Average q value-estimation: -0.5931524038314819\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "5     -306.008813    104.092957         -0.473706     5\n",
            "6     -169.018530    102.462624         -0.593152     6\n",
            "Training step: 71\n",
            "Training step: 72\n",
            "Training step: 73\n",
            "Training step: 74\n",
            "Training step: 75\n",
            "Training step: 76\n",
            "Training step: 77\n",
            "Training step: 78\n",
            "Training step: 79\n",
            "Training step: 80\n",
            "test_steps: 100 0.46875 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "TESTING: Average return: -204.43107366509417, Average loss: 147.52801513671875, Average q value-estimation: -0.803412914276123\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "5     -306.008813    104.092957         -0.473706     5\n",
            "6     -169.018530    102.462624         -0.593152     6\n",
            "7     -204.431074    147.528015         -0.803413     7\n",
            "Training step: 81\n",
            "Training step: 82\n",
            "Training step: 83\n",
            "Training step: 84\n",
            "Training step: 85\n",
            "Training step: 86\n",
            "Training step: 87\n",
            "Training step: 88\n",
            "Training step: 89\n",
            "Training step: 90\n",
            "test_steps: 100 0.3125 (64,) (64,)\n",
            "test_steps: 200 0.84375 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -143.15781896230044, Average loss: 147.6091766357422, Average q value-estimation: -1.1426804065704346\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "5     -306.008813    104.092957         -0.473706     5\n",
            "6     -169.018530    102.462624         -0.593152     6\n",
            "7     -204.431074    147.528015         -0.803413     7\n",
            "8     -143.157819    147.609177         -1.142680     8\n",
            "Training step: 91\n",
            "Training step: 92\n",
            "Training step: 93\n",
            "Training step: 94\n",
            "Training step: 95\n",
            "Training step: 96\n",
            "Training step: 97\n",
            "Training step: 98\n",
            "Training step: 99\n",
            "Training step: 100\n",
            "test_steps: 100 0.375 (64,) (64,)\n",
            "test_steps: 200 0.796875 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -141.5501940395805, Average loss: 101.76033782958984, Average q value-estimation: -1.4579203128814697\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0     -335.402921    119.788437          0.569178     0\n",
            "1     -306.230246    114.576294          1.083605     1\n",
            "2     -322.792646    103.765968          0.808594     2\n",
            "3     -302.531119     79.686661         -0.490919     3\n",
            "4     -273.120735    120.682045         -0.064719     4\n",
            "5     -306.008813    104.092957         -0.473706     5\n",
            "6     -169.018530    102.462624         -0.593152     6\n",
            "7     -204.431074    147.528015         -0.803413     7\n",
            "8     -143.157819    147.609177         -1.142680     8\n",
            "9     -141.550194    101.760338         -1.457920     9\n",
            "Training step: 101\n",
            "Training step: 102\n",
            "Training step: 103\n",
            "Training step: 104\n",
            "Training step: 105\n",
            "Training step: 106\n",
            "Training step: 107\n",
            "Training step: 108\n",
            "Training step: 109\n",
            "Training step: 110\n",
            "test_steps: 100 0.375 (64,) (64,)\n",
            "test_steps: 200 0.859375 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -78.28204858996489, Average loss: 71.58908081054688, Average q value-estimation: -1.423444390296936\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "Training step: 111\n",
            "Training step: 112\n",
            "Training step: 113\n",
            "Training step: 114\n",
            "Training step: 115\n",
            "Training step: 116\n",
            "Training step: 117\n",
            "Training step: 118\n",
            "Training step: 119\n",
            "Training step: 120\n",
            "test_steps: 100 0.34375 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "TESTING: Average return: -155.6070497934379, Average loss: 67.93931579589844, Average q value-estimation: -1.6629565954208374\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "Training step: 121\n",
            "Training step: 122\n",
            "Training step: 123\n",
            "Training step: 124\n",
            "Training step: 125\n",
            "Training step: 126\n",
            "Training step: 127\n",
            "Training step: 128\n",
            "Training step: 129\n",
            "Training step: 130\n",
            "test_steps: 100 0.390625 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -119.0645283204848, Average loss: 60.33002853393555, Average q value-estimation: -1.791099190711975\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "Training step: 131\n",
            "Training step: 132\n",
            "Training step: 133\n",
            "Training step: 134\n",
            "Training step: 135\n",
            "Training step: 136\n",
            "Training step: 137\n",
            "Training step: 138\n",
            "Training step: 139\n",
            "Training step: 140\n",
            "test_steps: 100 0.609375 (64,) (64,)\n",
            "test_steps: 200 1.0 (64,) (64,)\n",
            "TESTING: Average return: -200.14251896985118, Average loss: 89.16903686523438, Average q value-estimation: -2.5979607105255127\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "Training step: 141\n",
            "Training step: 142\n",
            "Training step: 143\n",
            "Training step: 144\n",
            "Training step: 145\n",
            "Training step: 146\n",
            "Training step: 147\n",
            "Training step: 148\n",
            "Training step: 149\n",
            "Training step: 150\n",
            "test_steps: 100 0.5 (64,) (64,)\n",
            "test_steps: 200 0.921875 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -236.73451512539643, Average loss: 99.19108581542969, Average q value-estimation: -2.866023302078247\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "Training step: 151\n",
            "Training step: 152\n",
            "Training step: 153\n",
            "Training step: 154\n",
            "Training step: 155\n",
            "Training step: 156\n",
            "Training step: 157\n",
            "Training step: 158\n",
            "Training step: 159\n",
            "Training step: 160\n",
            "test_steps: 100 0.296875 (64,) (64,)\n",
            "test_steps: 200 0.828125 (64,) (64,)\n",
            "test_steps: 300 0.921875 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -110.79650888419025, Average loss: 108.3465805053711, Average q value-estimation: -3.2935638427734375\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "Training step: 161\n",
            "Training step: 162\n",
            "Training step: 163\n",
            "Training step: 164\n",
            "Training step: 165\n",
            "Training step: 166\n",
            "Training step: 167\n",
            "Training step: 168\n",
            "Training step: 169\n",
            "Training step: 170\n",
            "test_steps: 100 0.515625 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -89.57551174275096, Average loss: 54.5919189453125, Average q value-estimation: -3.2249233722686768\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "Training step: 171\n",
            "Training step: 172\n",
            "Training step: 173\n",
            "Training step: 174\n",
            "Training step: 175\n",
            "Training step: 176\n",
            "Training step: 177\n",
            "Training step: 178\n",
            "Training step: 179\n",
            "Training step: 180\n",
            "test_steps: 100 0.625 (64,) (64,)\n",
            "test_steps: 200 0.9375 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -93.45310894871042, Average loss: 116.02978515625, Average q value-estimation: -4.163884162902832\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "Training step: 181\n",
            "Training step: 182\n",
            "Training step: 183\n",
            "Training step: 184\n",
            "Training step: 185\n",
            "Training step: 186\n",
            "Training step: 187\n",
            "Training step: 188\n",
            "Training step: 189\n",
            "Training step: 190\n",
            "test_steps: 100 0.25 (64,) (64,)\n",
            "test_steps: 200 0.890625 (64,) (64,)\n",
            "TESTING: Average return: -256.31730559460016, Average loss: 12.81378173828125, Average q value-estimation: -4.069782257080078\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "Training step: 191\n",
            "Training step: 192\n",
            "Training step: 193\n",
            "Training step: 194\n",
            "Training step: 195\n",
            "Training step: 196\n",
            "Training step: 197\n",
            "Training step: 198\n",
            "Training step: 199\n",
            "Training step: 200\n",
            "test_steps: 100 0.609375 (64,) (64,)\n",
            "TESTING: Average return: -78.21066091195081, Average loss: 143.3342742919922, Average q value-estimation: -4.824184417724609\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "Training step: 201\n",
            "Training step: 202\n",
            "Training step: 203\n",
            "Training step: 204\n",
            "Training step: 205\n",
            "Training step: 206\n",
            "Training step: 207\n",
            "Training step: 208\n",
            "Training step: 209\n",
            "Training step: 210\n",
            "test_steps: 100 0.28125 (64,) (64,)\n",
            "test_steps: 200 0.890625 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -110.74659880900995, Average loss: 134.7654266357422, Average q value-estimation: -5.993855953216553\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "Training step: 211\n",
            "Training step: 212\n",
            "Training step: 213\n",
            "Training step: 214\n",
            "Training step: 215\n",
            "Training step: 216\n",
            "Training step: 217\n",
            "Training step: 218\n",
            "Training step: 219\n",
            "Training step: 220\n",
            "test_steps: 100 0.421875 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -62.058012228416885, Average loss: 64.97208404541016, Average q value-estimation: -5.9395341873168945\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "Training step: 221\n",
            "Training step: 222\n",
            "Training step: 223\n",
            "Training step: 224\n",
            "Training step: 225\n",
            "Training step: 226\n",
            "Training step: 227\n",
            "Training step: 228\n",
            "Training step: 229\n",
            "Training step: 230\n",
            "test_steps: 100 0.46875 (64,) (64,)\n",
            "TESTING: Average return: -121.25769010928943, Average loss: 96.74327087402344, Average q value-estimation: -6.366945743560791\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "Training step: 231\n",
            "Training step: 232\n",
            "Training step: 233\n",
            "Training step: 234\n",
            "Training step: 235\n",
            "Training step: 236\n",
            "Training step: 237\n",
            "Training step: 238\n",
            "Training step: 239\n",
            "Training step: 240\n",
            "test_steps: 100 0.359375 (64,) (64,)\n",
            "test_steps: 200 0.84375 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -139.35773433099803, Average loss: 148.58868408203125, Average q value-estimation: -6.2837629318237305\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "Training step: 241\n",
            "Training step: 242\n",
            "Training step: 243\n",
            "Training step: 244\n",
            "Training step: 245\n",
            "Training step: 246\n",
            "Training step: 247\n",
            "Training step: 248\n",
            "Training step: 249\n",
            "Training step: 250\n",
            "test_steps: 100 0.515625 (64,) (64,)\n",
            "TESTING: Average return: -181.32391260435134, Average loss: 63.73713302612305, Average q value-estimation: -6.580300807952881\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "Training step: 251\n",
            "Training step: 252\n",
            "Training step: 253\n",
            "Training step: 254\n",
            "Training step: 255\n",
            "Training step: 256\n",
            "Training step: 257\n",
            "Training step: 258\n",
            "Training step: 259\n",
            "Training step: 260\n",
            "TESTING: Average return: -231.81324561708365, Average loss: 17.385326385498047, Average q value-estimation: -7.092947483062744\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "Training step: 261\n",
            "Training step: 262\n",
            "Training step: 263\n",
            "Training step: 264\n",
            "Training step: 265\n",
            "Training step: 266\n",
            "Training step: 267\n",
            "Training step: 268\n",
            "Training step: 269\n",
            "Training step: 270\n",
            "test_steps: 100 0.421875 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -86.09341946171958, Average loss: 132.35092163085938, Average q value-estimation: -7.661980628967285\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "Training step: 271\n",
            "Training step: 272\n",
            "Training step: 273\n",
            "Training step: 274\n",
            "Training step: 275\n",
            "Training step: 276\n",
            "Training step: 277\n",
            "Training step: 278\n",
            "Training step: 279\n",
            "Training step: 280\n",
            "test_steps: 100 0.296875 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -115.8677080197691, Average loss: 105.12252044677734, Average q value-estimation: -8.188041687011719\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "Training step: 281\n",
            "Training step: 282\n",
            "Training step: 283\n",
            "Training step: 284\n",
            "Training step: 285\n",
            "Training step: 286\n",
            "Training step: 287\n",
            "Training step: 288\n",
            "Training step: 289\n",
            "Training step: 290\n",
            "test_steps: 100 0.5625 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "TESTING: Average return: -119.56921136239959, Average loss: 48.45746994018555, Average q value-estimation: -8.745617866516113\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "Training step: 291\n",
            "Training step: 292\n",
            "Training step: 293\n",
            "Training step: 294\n",
            "Training step: 295\n",
            "Training step: 296\n",
            "Training step: 297\n",
            "Training step: 298\n",
            "Training step: 299\n",
            "Training step: 300\n",
            "test_steps: 100 0.546875 (64,) (64,)\n",
            "test_steps: 200 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -107.43597505784655, Average loss: 65.47380065917969, Average q value-estimation: -8.913528442382812\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "Training step: 301\n",
            "Training step: 302\n",
            "Training step: 303\n",
            "Training step: 304\n",
            "Training step: 305\n",
            "Training step: 306\n",
            "Training step: 307\n",
            "Training step: 308\n",
            "Training step: 309\n",
            "Training step: 310\n",
            "test_steps: 100 0.34375 (64,) (64,)\n",
            "test_steps: 200 0.78125 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -208.71287560423661, Average loss: 63.18739700317383, Average q value-estimation: -9.648805618286133\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "Training step: 311\n",
            "Training step: 312\n",
            "Training step: 313\n",
            "Training step: 314\n",
            "Training step: 315\n",
            "Training step: 316\n",
            "Training step: 317\n",
            "Training step: 318\n",
            "Training step: 319\n",
            "Training step: 320\n",
            "test_steps: 100 0.8125 (64,) (64,)\n",
            "TESTING: Average return: -64.59422053814833, Average loss: 55.79902267456055, Average q value-estimation: -9.66147518157959\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "Training step: 321\n",
            "Training step: 322\n",
            "Training step: 323\n",
            "Training step: 324\n",
            "Training step: 325\n",
            "Training step: 326\n",
            "Training step: 327\n",
            "Training step: 328\n",
            "Training step: 329\n",
            "Training step: 330\n",
            "test_steps: 100 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -302.40324933071037, Average loss: 104.10184478759766, Average q value-estimation: -10.377542495727539\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "Training step: 331\n",
            "Training step: 332\n",
            "Training step: 333\n",
            "Training step: 334\n",
            "Training step: 335\n",
            "Training step: 336\n",
            "Training step: 337\n",
            "Training step: 338\n",
            "Training step: 339\n",
            "Training step: 340\n",
            "test_steps: 100 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -23.318323077880258, Average loss: 96.04925537109375, Average q value-estimation: -10.900558471679688\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "Training step: 341\n",
            "Training step: 342\n",
            "Training step: 343\n",
            "Training step: 344\n",
            "Training step: 345\n",
            "Training step: 346\n",
            "Training step: 347\n",
            "Training step: 348\n",
            "Training step: 349\n",
            "Training step: 350\n",
            "test_steps: 100 0.375 (64,) (64,)\n",
            "test_steps: 200 0.953125 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -80.54728392874044, Average loss: 156.9518280029297, Average q value-estimation: -11.251932144165039\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "Training step: 351\n",
            "Training step: 352\n",
            "Training step: 353\n",
            "Training step: 354\n",
            "Training step: 355\n",
            "Training step: 356\n",
            "Training step: 357\n",
            "Training step: 358\n",
            "Training step: 359\n",
            "Training step: 360\n",
            "test_steps: 100 0.328125 (64,) (64,)\n",
            "test_steps: 200 0.859375 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -118.3860125128434, Average loss: 70.90013885498047, Average q value-estimation: -11.91901969909668\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "Training step: 361\n",
            "Training step: 362\n",
            "Training step: 363\n",
            "Training step: 364\n",
            "Training step: 365\n",
            "Training step: 366\n",
            "Training step: 367\n",
            "Training step: 368\n",
            "Training step: 369\n",
            "Training step: 370\n",
            "test_steps: 100 0.359375 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -66.62200463098014, Average loss: 85.48503875732422, Average q value-estimation: -11.53422737121582\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "Training step: 371\n",
            "Training step: 372\n",
            "Training step: 373\n",
            "Training step: 374\n",
            "Training step: 375\n",
            "Training step: 376\n",
            "Training step: 377\n",
            "Training step: 378\n",
            "Training step: 379\n",
            "Training step: 380\n",
            "test_steps: 100 0.40625 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "test_steps: 400 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -107.46425341899042, Average loss: 139.63717651367188, Average q value-estimation: -11.996160507202148\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "Training step: 381\n",
            "Training step: 382\n",
            "Training step: 383\n",
            "Training step: 384\n",
            "Training step: 385\n",
            "Training step: 386\n",
            "Training step: 387\n",
            "Training step: 388\n",
            "Training step: 389\n",
            "Training step: 390\n",
            "test_steps: 100 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -8.94649546001358, Average loss: 168.73019409179688, Average q value-estimation: -12.070486068725586\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "Training step: 391\n",
            "Training step: 392\n",
            "Training step: 393\n",
            "Training step: 394\n",
            "Training step: 395\n",
            "Training step: 396\n",
            "Training step: 397\n",
            "Training step: 398\n",
            "Training step: 399\n",
            "Training step: 400\n",
            "test_steps: 100 0.921875 (64,) (64,)\n",
            "TESTING: Average return: -36.02963991478557, Average loss: 124.9389877319336, Average q value-estimation: -12.761518478393555\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "Training step: 401\n",
            "Training step: 402\n",
            "Training step: 403\n",
            "Training step: 404\n",
            "Training step: 405\n",
            "Training step: 406\n",
            "Training step: 407\n",
            "Training step: 408\n",
            "Training step: 409\n",
            "Training step: 410\n",
            "test_steps: 100 0.5 (64,) (64,)\n",
            "test_steps: 200 0.9375 (64,) (64,)\n",
            "TESTING: Average return: -111.3231247211366, Average loss: 38.99062728881836, Average q value-estimation: -12.651094436645508\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "Training step: 411\n",
            "Training step: 412\n",
            "Training step: 413\n",
            "Training step: 414\n",
            "Training step: 415\n",
            "Training step: 416\n",
            "Training step: 417\n",
            "Training step: 418\n",
            "Training step: 419\n",
            "Training step: 420\n",
            "test_steps: 100 0.515625 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -59.88534584184972, Average loss: 70.57723236083984, Average q value-estimation: -13.618505477905273\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "Training step: 421\n",
            "Training step: 422\n",
            "Training step: 423\n",
            "Training step: 424\n",
            "Training step: 425\n",
            "Training step: 426\n",
            "Training step: 427\n",
            "Training step: 428\n",
            "Training step: 429\n",
            "Training step: 430\n",
            "test_steps: 100 0.3125 (64,) (64,)\n",
            "test_steps: 200 0.890625 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -89.771609788342, Average loss: 143.71725463867188, Average q value-estimation: -14.412005424499512\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "Training step: 431\n",
            "Training step: 432\n",
            "Training step: 433\n",
            "Training step: 434\n",
            "Training step: 435\n",
            "Training step: 436\n",
            "Training step: 437\n",
            "Training step: 438\n",
            "Training step: 439\n",
            "Training step: 440\n",
            "test_steps: 100 0.796875 (64,) (64,)\n",
            "test_steps: 200 0.984375 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -26.121341610779382, Average loss: 98.38206481933594, Average q value-estimation: -14.53801441192627\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "Training step: 441\n",
            "Training step: 442\n",
            "Training step: 443\n",
            "Training step: 444\n",
            "Training step: 445\n",
            "Training step: 446\n",
            "Training step: 447\n",
            "Training step: 448\n",
            "Training step: 449\n",
            "Training step: 450\n",
            "test_steps: 100 0.453125 (64,) (64,)\n",
            "test_steps: 200 0.9375 (64,) (64,)\n",
            "TESTING: Average return: -62.970567063060265, Average loss: 136.33218383789062, Average q value-estimation: -14.577107429504395\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "Training step: 451\n",
            "Training step: 452\n",
            "Training step: 453\n",
            "Training step: 454\n",
            "Training step: 455\n",
            "Training step: 456\n",
            "Training step: 457\n",
            "Training step: 458\n",
            "Training step: 459\n",
            "Training step: 460\n",
            "test_steps: 100 0.28125 (64,) (64,)\n",
            "test_steps: 200 0.90625 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -97.30253741037133, Average loss: 60.40888595581055, Average q value-estimation: -14.818801879882812\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "45      -97.302537     60.408886        -14.818802    45\n",
            "Training step: 461\n",
            "Training step: 462\n",
            "Training step: 463\n",
            "Training step: 464\n",
            "Training step: 465\n",
            "Training step: 466\n",
            "Training step: 467\n",
            "Training step: 468\n",
            "Training step: 469\n",
            "Training step: 470\n",
            "test_steps: 100 0.703125 (64,) (64,)\n",
            "TESTING: Average return: -325.8976619229702, Average loss: 132.0396270751953, Average q value-estimation: -15.809870719909668\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "45      -97.302537     60.408886        -14.818802    45\n",
            "46     -325.897662    132.039627        -15.809871    46\n",
            "Training step: 471\n",
            "Training step: 472\n",
            "Training step: 473\n",
            "Training step: 474\n",
            "Training step: 475\n",
            "Training step: 476\n",
            "Training step: 477\n",
            "Training step: 478\n",
            "Training step: 479\n",
            "Training step: 480\n",
            "test_steps: 100 0.390625 (64,) (64,)\n",
            "test_steps: 200 0.875 (64,) (64,)\n",
            "test_steps: 300 0.984375 (64,) (64,)\n",
            "test_steps: 400 0.984375 (64,) (64,)\n",
            "test_steps: 500 0.984375 (64,) (64,)\n",
            "TESTING: Average return: -45.04892289093998, Average loss: 33.189727783203125, Average q value-estimation: -15.981203079223633\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "45      -97.302537     60.408886        -14.818802    45\n",
            "46     -325.897662    132.039627        -15.809871    46\n",
            "47      -45.048923     33.189728        -15.981203    47\n",
            "Training step: 481\n",
            "Training step: 482\n",
            "Training step: 483\n",
            "Training step: 484\n",
            "Training step: 485\n",
            "Training step: 486\n",
            "Training step: 487\n",
            "Training step: 488\n",
            "Training step: 489\n",
            "Training step: 490\n",
            "test_steps: 100 0.796875 (64,) (64,)\n",
            "test_steps: 200 0.953125 (64,) (64,)\n",
            "TESTING: Average return: -29.741039182721252, Average loss: 89.08784484863281, Average q value-estimation: -16.28059196472168\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "45      -97.302537     60.408886        -14.818802    45\n",
            "46     -325.897662    132.039627        -15.809871    46\n",
            "47      -45.048923     33.189728        -15.981203    47\n",
            "48      -29.741039     89.087845        -16.280592    48\n",
            "Training step: 491\n",
            "Training step: 492\n",
            "Training step: 493\n",
            "Training step: 494\n",
            "Training step: 495\n",
            "Training step: 496\n",
            "Training step: 497\n",
            "Training step: 498\n",
            "Training step: 499\n",
            "test_steps: 100 0.421875 (64,) (64,)\n",
            "test_steps: 200 0.890625 (64,) (64,)\n",
            "test_steps: 300 0.96875 (64,) (64,)\n",
            "TESTING: Average return: -71.66880478679354, Average loss: 77.49632263183594, Average q value-estimation: -16.828155517578125\n",
            "    average_return  average_loss  average_q_values  step\n",
            "0      -335.402921    119.788437          0.569178     0\n",
            "1      -306.230246    114.576294          1.083605     1\n",
            "2      -322.792646    103.765968          0.808594     2\n",
            "3      -302.531119     79.686661         -0.490919     3\n",
            "4      -273.120735    120.682045         -0.064719     4\n",
            "5      -306.008813    104.092957         -0.473706     5\n",
            "6      -169.018530    102.462624         -0.593152     6\n",
            "7      -204.431074    147.528015         -0.803413     7\n",
            "8      -143.157819    147.609177         -1.142680     8\n",
            "9      -141.550194    101.760338         -1.457920     9\n",
            "10      -78.282049     71.589081         -1.423444    10\n",
            "11     -155.607050     67.939316         -1.662957    11\n",
            "12     -119.064528     60.330029         -1.791099    12\n",
            "13     -200.142519     89.169037         -2.597961    13\n",
            "14     -236.734515     99.191086         -2.866023    14\n",
            "15     -110.796509    108.346581         -3.293564    15\n",
            "16      -89.575512     54.591919         -3.224923    16\n",
            "17      -93.453109    116.029785         -4.163884    17\n",
            "18     -256.317306     12.813782         -4.069782    18\n",
            "19      -78.210661    143.334274         -4.824184    19\n",
            "20     -110.746599    134.765427         -5.993856    20\n",
            "21      -62.058012     64.972084         -5.939534    21\n",
            "22     -121.257690     96.743271         -6.366946    22\n",
            "23     -139.357734    148.588684         -6.283763    23\n",
            "24     -181.323913     63.737133         -6.580301    24\n",
            "25     -231.813246     17.385326         -7.092947    25\n",
            "26      -86.093419    132.350922         -7.661981    26\n",
            "27     -115.867708    105.122520         -8.188042    27\n",
            "28     -119.569211     48.457470         -8.745618    28\n",
            "29     -107.435975     65.473801         -8.913528    29\n",
            "30     -208.712876     63.187397         -9.648806    30\n",
            "31      -64.594221     55.799023         -9.661475    31\n",
            "32     -302.403249    104.101845        -10.377542    32\n",
            "33      -23.318323     96.049255        -10.900558    33\n",
            "34      -80.547284    156.951828        -11.251932    34\n",
            "35     -118.386013     70.900139        -11.919020    35\n",
            "36      -66.622005     85.485039        -11.534227    36\n",
            "37     -107.464253    139.637177        -11.996161    37\n",
            "38       -8.946495    168.730194        -12.070486    38\n",
            "39      -36.029640    124.938988        -12.761518    39\n",
            "40     -111.323125     38.990627        -12.651094    40\n",
            "41      -59.885346     70.577232        -13.618505    41\n",
            "42      -89.771610    143.717255        -14.412005    42\n",
            "43      -26.121342     98.382065        -14.538014    43\n",
            "44      -62.970567    136.332184        -14.577107    44\n",
            "45      -97.302537     60.408886        -14.818802    45\n",
            "46     -325.897662    132.039627        -15.809871    46\n",
            "47      -45.048923     33.189728        -15.981203    47\n",
            "48      -29.741039     89.087845        -16.280592    48\n",
            "49      -71.668805     77.496323        -16.828156    49\n"
          ]
        }
      ],
      "source": [
        "dqn()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "virtualenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}