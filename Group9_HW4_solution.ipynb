{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syntactic/DeepReinforcementLearning/blob/main/Group9_HW4_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 Homework Review\n",
        "This task asks you to review two other groups’ homework. The goal includes (1) for you to get a better understanding of contents by reviewing other groups submissions, (2) helping them understand how they could improve with their code (and possibly RL), and (3) help you improve by receiving valuable feedback from other groups. Step-by-step:\n",
        "1. Coordinate with two other groups for mutual feedback. You may use the forum to achieve this, but we also try to match groups spontaneously at each QnA.\n",
        "1\n",
        "2. Take 15-30 min each to review their respective submissions. Write bullet points on your findings (both what your group should learn from their submission, and what the other group should improve)\n",
        "3. Get together and discuss this feedback with representatives of all three groups in one of either the in-person or digital QnA sessions. Have one of the attending tutors as a ’referee’ for any upcoming discussion and questions, and make sure they write down having refereed your group.\n",
        "4. Denote the groups and respective tutor in the homework submission form"
      ],
      "metadata": {
        "id": "40il4QHk9EgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2 DQN\n",
        "This homework asks you to implement DQN on the [ALE Breakout v5 Atari Game](https://gymnasium.farama.org/environments/atari/breakout/)\n",
        "* Achieving a reasonable score takes four to twenty four hours on a reasonable computer system with dedicated GPU. plan accordingly and build towards efficiency and throughput (make use of vectorized environments!).\n",
        "* If you do not have the necessary compute ressources available, you may instead solve the discrete version of lunar lander (also available on gym- nasium!). Notice you can not be awarded an outstanding in this case however!\n",
        "* Make use of the [Implementing DQN from scratch video](https://www.youtube.com/playlist?list=PLPitqsshnVV8YOGE1r-Sm2zVtuSsGNL_G) series if necessary.\n",
        "* Make use of a delayed target network, prefilling the ERP and some additional measures to tackle the overestimation bias (e.g. Double DQN)"
      ],
      "metadata": {
        "id": "Rz-H7Rur9dHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium[atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "uvBpU5WD9eFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba265ee9-4b79-4268-8780-306fe684ee9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[accept-rom-license,atari]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=2ad32065ae4b7cc6230ba0e6c5ef311067d2d576d334a0fba98cba7bdd2a5d18\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, jax-jumpy, ale-py, gymnasium, AutoROM.accept-rom-license, autorom, shimmy\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "fVPZDFA_OxrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Breakout-v5\")\n",
        "obs, _ = env.reset()"
      ],
      "metadata": {
        "id": "_KEdv7IlO5JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayBuffer:\n",
        "\n",
        "    def __init__(self, max_size: int, environment_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps:int):\n",
        "        self.max_size = max_size\n",
        "        self.environment_name = environment_name\n",
        "        self.parallel_game_unrolls = parallel_game_unrolls\n",
        "        self.unroll_steps = unroll_steps\n",
        "        self.observation_preprocessing_function = observation_preprocessing_function\n",
        "        self.num_possible_actions = self.env.single_action_space.n\n",
        "        self.envs = gym.vector.make(environment_name, num_envs=self.parallel_game_unrolls)\n",
        "        self.current_states, _ = self.envs.reset()\n",
        "        self.data = []\n",
        "\n",
        "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "        # adds new samples into the ERP\n",
        "\n",
        "        states_list = []\n",
        "        actions_list = []\n",
        "        rewards_list = []\n",
        "        terminateds_list = []\n",
        "        next_states_list = []\n",
        "\n",
        "        \n",
        "        for i in range(self.unroll_steps):\n",
        "            actions = self.sample_epsilon_greedy(dqn_network, epsilon)\n",
        "            # take the action and get s' and r\n",
        "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "            # store observation, action, reward, next_observation into ERP container\n",
        "            #\n",
        "            states_list.append(self.current_states)\n",
        "            actions_list.append(actions)\n",
        "            rewards_list.append(rewards)\n",
        "            terminateds_list.append(terminateds)\n",
        "            next_states_list.append(next_states)\n",
        "            self.current_states = next_states\n",
        "\n",
        "        def data_generator():\n",
        "            for states_batch, actions_batch, rewards_batch, terminateds_batch, next_states_batch in \\\n",
        "                zip(states_list, actions_list, rewards_list, terminateds_list, next_states_list):\n",
        "                for game_idx in range(self.parallel_game_unrolls):\n",
        "                    state = states_batch[game_idx,:,:,:]\n",
        "                    action = actions_batch[game_idx]\n",
        "                    reward = rewards_batch[game_idx]\n",
        "                    terminated = terminateds_batch[game_idx]\n",
        "                    next_state = next_states_batch[game_idx,:,:,:]\n",
        "            yield(state, action, reward, next_state, terminated)\n",
        "        \n",
        "        dataset_tensor_specs = (tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8), \n",
        "                                tf.TensorSpec(shape=(), dtype=tf.int32), \n",
        "                                tf.TensorSpec(shape=(), dtype=tf.float32), \n",
        "                                tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.bool))\n",
        "\n",
        "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "        \n",
        "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, next_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(next_state), terminated))\n",
        "        new_samples_dataset = new_samples_dataset.cache().shuffle(buffer_size=self.unroll_steps * self.parallel_game_unrolls, reshuffle_each_iteration=True)\n",
        "\n",
        "        for elem in new_samples_dataset:\n",
        "            continue\n",
        "\n",
        "        self.data.append(new_samples_dataset)\n",
        "\n",
        "        if(len(self.data) * self.parallel_game_unrolls * self.unroll_steps > self.max_size):\n",
        "            self.data.pop(0)\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        ERP_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False)\n",
        "        return ERP_dataset\n",
        "\n",
        "    def sample_epsilon_greedy(self, dqn_network, epsilon):\n",
        "        observations = self.observation_preprocessing_function(self.current_states)\n",
        "        q_values = dqn_network(observations) # tensor float 32 shape(parallel_game_unrolls, num_actions)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1)\n",
        "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions)\n",
        "        return actions\n",
        "\n",
        "def observation_preprocessing_function(observation):\n",
        "    # preprocess our observation so that it has shape (84, 84) and is between -1 and 1\n",
        "    observation = tf.image.reseize(observation, shape=(84,84))\n",
        "    observation = tf.cast(observation, dtype=tf.float32)/128.0 - 1.0\n",
        "    return observation\n",
        "\n",
        "def create_dqn_model(num_actions: int):\n",
        "    # create intput for function tf model api\n",
        "    input_layer = tf.keras.Input(shape=(84,84,3), dtype=tf.float32)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer) + x # residual connections\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer) + x\n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(input_layer) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(input_layer) + x\n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer) + x \n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(input_layer) + x\n",
        "\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64, activation='relu')(x) + x\n",
        "    x = tf.keras.layers.Dense(units=num_actions, activations='linear')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, ouputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9AmYtKI8fPz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = env.action_space\n",
        "for i in range(1000):\n",
        "  action = actions.sample()\n",
        "  obs, reward, terminated, truncated, info = env.step(action)\n",
        "  if terminated:\n",
        "    print(i)\n",
        "    env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXPFIBW3PFCK",
        "outputId": "23dd639f-f1d9-4849-a9c6-3d3eb79be536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "238\n",
            "373\n",
            "582\n",
            "748\n",
            "880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1QPITEOv45q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}