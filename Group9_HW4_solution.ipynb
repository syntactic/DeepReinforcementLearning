{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syntactic/DeepReinforcementLearning/blob/main/Group9_HW4_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40il4QHk9EgU"
      },
      "source": [
        "### 1 Homework Review\n",
        "This task asks you to review two other groups’ homework. The goal includes (1) for you to get a better understanding of contents by reviewing other groups submissions, (2) helping them understand how they could improve with their code (and possibly RL), and (3) help you improve by receiving valuable feedback from other groups. Step-by-step:\n",
        "1. Coordinate with two other groups for mutual feedback. You may use the forum to achieve this, but we also try to match groups spontaneously at each QnA.\n",
        "1\n",
        "2. Take 15-30 min each to review their respective submissions. Write bullet points on your findings (both what your group should learn from their submission, and what the other group should improve)\n",
        "3. Get together and discuss this feedback with representatives of all three groups in one of either the in-person or digital QnA sessions. Have one of the attending tutors as a ’referee’ for any upcoming discussion and questions, and make sure they write down having refereed your group.\n",
        "4. Denote the groups and respective tutor in the homework submission form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz-H7Rur9dHH"
      },
      "source": [
        "\n",
        "### 2 DQN\n",
        "This homework asks you to implement DQN on the [ALE Breakout v5 Atari Game](https://gymnasium.farama.org/environments/atari/breakout/)\n",
        "* Achieving a reasonable score takes four to twenty four hours on a reasonable computer system with dedicated GPU. plan accordingly and build towards efficiency and throughput (make use of vectorized environments!).\n",
        "* If you do not have the necessary compute ressources available, you may instead solve the discrete version of lunar lander (also available on gym- nasium!). Notice you can not be awarded an outstanding in this case however!\n",
        "* Make use of the [Implementing DQN from scratch video](https://www.youtube.com/playlist?list=PLPitqsshnVV8YOGE1r-Sm2zVtuSsGNL_G) series if necessary.\n",
        "* Make use of a delayed target network, prefilling the ERP and some additional measures to tackle the overestimation bias (e.g. Double DQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvBpU5WD9eFF",
        "outputId": "9b304e67-1fbd-4b05-d170-ba91be34a142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[accept-rom-license,atari]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=01f0266912c142f8cf99719ea949b6f528c3599566122350e357c9e0e49997dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, jax-jumpy, ale-py, gymnasium, AutoROM.accept-rom-license, autorom, shimmy\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 shimmy-0.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fVPZDFA_OxrN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9AmYtKI8fPz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094998c9-28ea-4ec2-91f5-6aa6bf83b212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class ExperienceReplayBuffer:\n",
        "\n",
        "    def __init__(self, max_size: int, environment_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps:int):\n",
        "        self.max_size = max_size\n",
        "        self.environment_name = environment_name\n",
        "        self.parallel_game_unrolls = parallel_game_unrolls\n",
        "        self.unroll_steps = unroll_steps\n",
        "        self.observation_preprocessing_function = observation_preprocessing_function\n",
        "        self.num_possible_actions = gym.make(environment_name).action_space.n\n",
        "        self.envs = gym.vector.make(environment_name, num_envs=self.parallel_game_unrolls)\n",
        "        self.current_states, _ = self.envs.reset()\n",
        "        self.data = []\n",
        "\n",
        "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "        # adds new samples into the ERP\n",
        "\n",
        "        states_list = []\n",
        "        actions_list = []\n",
        "        rewards_list = []\n",
        "        terminateds_list = []\n",
        "        next_states_list = []\n",
        "\n",
        "        \n",
        "        for i in range(self.unroll_steps):\n",
        "            actions = self.sample_epsilon_greedy(dqn_network, epsilon) # (PARALLEL_GAME_UNROLLS, )\n",
        "            # take the action and get s' and r\n",
        "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "            # store observation, action, reward, next_observation into ERP container\n",
        "            #\n",
        "            states_list.append(self.current_states)\n",
        "            actions_list.append(actions)\n",
        "            rewards_list.append(rewards)\n",
        "            terminateds_list.append(terminateds)\n",
        "            next_states_list.append(next_states)\n",
        "            self.current_states = next_states\n",
        "\n",
        "        def data_generator():\n",
        "            for states_batch, actions_batch, rewards_batch, terminateds_batch, next_states_batch in \\\n",
        "                zip(states_list, actions_list, rewards_list, terminateds_list, next_states_list):\n",
        "                for game_idx in range(self.parallel_game_unrolls):\n",
        "                    state = states_batch[game_idx,:,:,:]\n",
        "                    action = actions_batch[game_idx]\n",
        "                    reward = rewards_batch[game_idx]\n",
        "                    terminated = terminateds_batch[game_idx]\n",
        "                    next_state = next_states_batch[game_idx,:,:,:]\n",
        "                    yield(state, action, reward, next_state, terminated)\n",
        "        \n",
        "        dataset_tensor_specs = (tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8), \n",
        "                                tf.TensorSpec(shape=(), dtype=tf.int32), \n",
        "                                tf.TensorSpec(shape=(), dtype=tf.float32), \n",
        "                                tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.bool))\n",
        "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "        \n",
        "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, next_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(next_state), terminated))\n",
        "        new_samples_dataset = new_samples_dataset.cache().shuffle(buffer_size=self.unroll_steps * self.parallel_game_unrolls, reshuffle_each_iteration=True)\n",
        "\n",
        "        for elem in new_samples_dataset:\n",
        "            continue\n",
        "\n",
        "        self.data.append(new_samples_dataset)\n",
        "\n",
        "        datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps\n",
        "        if datapoints_in_data > self.max_size:\n",
        "            self.data.pop(0)\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        ERP_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False)\n",
        "        return ERP_dataset\n",
        "\n",
        "    def sample_epsilon_greedy(self, dqn_network, epsilon):\n",
        "        observations = self.observation_preprocessing_function(self.current_states)\n",
        "        q_values = dqn_network(observations) # tensor float 32 shape(parallel_game_unrolls, num_actions)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1)\n",
        "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy()\n",
        "        return actions\n",
        "\n",
        "def observation_preprocessing_function(observation):\n",
        "    # preprocess our observation so that it has shape (84, 84) and is between -1 and 1\n",
        "    observation = tf.image.resize(observation, size=(84,84))\n",
        "    observation = tf.cast(observation, dtype=tf.float32)/128.0 - 1.0\n",
        "    return observation\n",
        "\n",
        "def create_dqn_model(num_actions: int):\n",
        "    # create intput for function tf model api\n",
        "    input_layer = tf.keras.Input(shape=(84,84,3), dtype=tf.float32)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) # (84, 84, 3)\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) + x # residual connections\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(input_layer) + x\n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) # (42, 42, )\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) # (42, 42, )\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) #(21, 21, )\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) #(21, 21, )\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x \n",
        "\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x) #(10, 10, )\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x\n",
        "\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64, activation='relu')(x) + x\n",
        "    x = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_dqn(train_dqn_network, target_network, dataset, optimizer, gamma: float, num_training_steps: int, batch_size: int=256):\n",
        "    dataset = dataset.batch(batch_size).prefetch(4)\n",
        "    @tf.function\n",
        "    def training_step(q_target, observations, actions):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_predictions_all_actions = train_dqn_network(observations) # shape of q_predictions is (batch_size, num_actions)\n",
        "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1)\n",
        "            loss = tf.reduce_mean(tf.square(q_predictions - q_target))\n",
        "        gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    losses = []\n",
        "    q_values = []\n",
        "    for i, state_transition in enumerate(dataset):\n",
        "        state, action, reward, subsequent_state, terminated = state_transition\n",
        "        # calculate q_target\n",
        "        #print(subsequent_state.shape)\n",
        "        q_vals = target_network(subsequent_state)\n",
        "        q_values.append(q_vals.numpy())\n",
        "        max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "        use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "        q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
        "        loss = training_step(q_target, observations=state, actions=action)\n",
        "        losses.append(loss)\n",
        "        if i >= num_training_steps:\n",
        "            break\n",
        "    return np.mean(losses), np.mean(q_values)\n",
        "\n",
        "def test_q_network(test_dqn_network, environment_name: str, num_parallel_tests: int, gamma: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
        "    envs = gym.vector.make(environment_name, num_parallel_tests)\n",
        "    num_possible_actions = envs.single_action_space.n\n",
        "    states, _ = envs.reset()\n",
        "    done = False\n",
        "    timestep = 0\n",
        "    # episodes_finished is np vector of shape (num_parallel_tests,), filled with booleans, starting with all False\n",
        "    episodes_finished = np.zeros(num_parallel_tests, dtype=bool)\n",
        "    returns = np.zeros(num_parallel_tests)\n",
        "    test_steps = 0\n",
        "    while not done:\n",
        "        states = preprocessing_function(states)\n",
        "        q_values = test_dqn_network(states)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_tests,)\n",
        "        random_actions = tf.random.uniform(shape=(num_parallel_tests, ), minval=0,\n",
        "                                           maxval=num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_tests,), minval=0,\n",
        "                                             maxval=1, dtype=tf.float32) > test_epsilon # tensor of type tf.bool, shape (num_parallel_tests,)\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (num_parallel_tests,)\n",
        "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
        "        # compute pointwise or between episodes_finished and terminateds\n",
        "        episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
        "        returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32))\n",
        "        timestep += 1\n",
        "        # done if all episodes are finished\n",
        "        done = np.all(episodes_finished)\n",
        "        test_steps += 1\n",
        "        if test_steps % 100 == 0:\n",
        "           print(f\"test_steps: {test_steps} {np.sum(episodes_finished)/num_parallel_tests} {terminateds.shape} {episodes_finished.shape}\")\n",
        "    return np.mean(returns)\n",
        "\n",
        "def visualize_q_values(results_df, step):\n",
        "    # create three subplots\n",
        "    fig, axis = plt.subplots(1, 3)\n",
        "    # include the row idxs explicitly in the results_df\n",
        "    results_df['step'] = results_df.index\n",
        "    # plot the average return\n",
        "    sns.lineplot(x='step', y='average_return', data=results_df, ax=axis[0])\n",
        "    # plot the average loss\n",
        "    sns.lineplot(x='step', y='average_loss', data=results_df, ax=axis[1])\n",
        "    # plot the average q values\n",
        "    sns.lineplot(x='step', y='average_q_values', data=results_df, ax=axis[2])\n",
        "    # save the figure\n",
        "    # create a timestring from the timestamp\n",
        "    timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # save the figure\n",
        "    plt.savefig(f\"./{timestring}_results_step{step}.png\")\n",
        "    # close the figure\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def polyak_averaging_weights(source_network, target_network, polyak_averaging_factor: float):\n",
        "    source_network_weights = source_network.get_weights()\n",
        "    target_network_weights = target_network.get_weights()\n",
        "    averaged_weights = []\n",
        "    for source_weight, target_weight in zip(source_network_weights, target_network_weights):\n",
        "        fraction_kept_weights = polyak_averaging_factor * target_weight\n",
        "        fraction_updated_weights = (1-polyak_averaging_factor) * source_weight\n",
        "        averaged_weight = fraction_kept_weights + fraction_updated_weights\n",
        "        averaged_weights.append(averaged_weight)\n",
        "    target_network.set_weights(averaged_weights)\n",
        "\n",
        "def dqn():\n",
        "    ENVIRONMENT_NAME = \"ALE/Breakout-v5\"\n",
        "    NUMBER_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "    ERP_SIZE = 10000\n",
        "    PARALLEL_GAME_UNROLLS = 64\n",
        "    UNROLL_STEPS = 4\n",
        "    EPSILON = 0.2\n",
        "    GAMMA = 0.98\n",
        "    NUM_TRAINING_STEPS_PER_ITER = 4\n",
        "    NUM_TRAINING_ITERS = 50000\n",
        "    TEST_EVERY_N_STEPS = 1000\n",
        "    TEST_NUM_PARALLEL_ENVS = 128\n",
        "    PREFILL_STEPS = 50\n",
        "    POLYAK_AVERAGING_FACTOR = 0.99\n",
        "    erp = ExperienceReplayBuffer(max_size=ERP_SIZE, environment_name=ENVIRONMENT_NAME, \n",
        "                                 parallel_game_unrolls=PARALLEL_GAME_UNROLLS, unroll_steps=UNROLL_STEPS,\n",
        "                                 observation_preprocessing_function=observation_preprocessing_function)\n",
        "    \n",
        "    # This is the DQN we train\n",
        "    dqn_agent = create_dqn_model(num_actions=NUMBER_ACTIONS)\n",
        "    # This is the target network, used to calculate the q-estimation targets\n",
        "    target_network = create_dqn_model(num_actions=NUMBER_ACTIONS)\n",
        "    dqn_agent.summary()\n",
        "    # test the agent\n",
        "    dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
        "    # copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.0\n",
        "    polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=0.0)\n",
        "\n",
        "    dqn_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    return_tracker = []\n",
        "    dqn_prediction_error = []\n",
        "    average_q_values = []\n",
        "\n",
        "    # prefill the replay buffer\n",
        "    prefill_exploration_epsilon = 1.\n",
        "    for prefill_step in range(PREFILL_STEPS):\n",
        "        erp.fill_with_samples(dqn_agent, epsilon=prefill_exploration_epsilon)\n",
        "\n",
        "\n",
        "    for step in range(NUM_TRAINING_ITERS):\n",
        "        print(f'Training step: {step}')\n",
        "        # step 1: put some s, a, r, s', t transitions into the replay buffer\n",
        "        erp.fill_with_samples(dqn_agent, epsilon=EPSILON)\n",
        "        dataset = erp.create_dataset()\n",
        "        # step 2: train on some samples from the replay buffer\n",
        "        average_loss, average_q_vals = train_dqn(dqn_agent, target_network, dataset, dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITER)\n",
        "        # update the target network via polyak averaging\n",
        "        polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "        if step % TEST_EVERY_N_STEPS == 0:\n",
        "            average_return = test_q_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA,\n",
        "                                            preprocessing_function=observation_preprocessing_function)\n",
        "            return_tracker.append(average_return)\n",
        "            dqn_prediction_error.append(average_loss)\n",
        "            average_q_values.append(average_q_vals)\n",
        "            # print average returns, average loss, average q values\n",
        "            print(f\"TESTING: Average return: {average_return}, Average loss: {average_loss}, Average q value-estimation: {average_q_vals}\")\n",
        "            # put all result lists into a dataframe by transforming them into a dict first\n",
        "            results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values}\n",
        "            results_df = pd.DataFrame(results_dict)\n",
        "            # visualize the results with sns\n",
        "            visualize_q_values(results_df, step)\n",
        "            print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1QPITEOv45q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2f12e5-2072-4101-80ea-913de4274c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 84, 84, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 84, 84, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 84, 84, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 84, 84, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 84, 84, 16)  0           ['conv2d_1[0][0]',               \n",
            " da)                                                              'conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 84, 84, 16)  0           ['conv2d_2[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 42, 42, 16)   0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 42, 42, 32)   4640        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 42, 42, 32)   9248        ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 42, 42, 32)  0           ['conv2d_4[0][0]',               \n",
            " mbda)                                                            'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 42, 42, 32)   9248        ['tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 42, 42, 32)  0           ['conv2d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 21, 21, 32)  0           ['tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 21, 21, 64)   18496       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 21, 21, 64)   36928       ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 21, 21, 64)  0           ['conv2d_7[0][0]',               \n",
            " mbda)                                                            'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 21, 21, 64)   36928       ['tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 21, 21, 64)  0           ['conv2d_8[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 64)  0           ['tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 10, 10, 64)   36928       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 10, 10, 64)  0           ['conv2d_9[0][0]',               \n",
            " mbda)                                                            'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 10, 10, 64)   36928       ['tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 10, 10, 64)  0           ['conv2d_10[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 10, 10, 64)   36928       ['tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 10, 10, 64)  0           ['conv2d_11[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 64)          0           ['tf.__operators__.add_8[0][0]'] \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           4160        ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 64)          0           ['dense[0][0]',                  \n",
            " mbda)                                                            'global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 4)            260         ['tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 232,036\n",
            "Trainable params: 232,036\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Training step: 0\n",
            "test_steps: 100 0.0 (128,) (128,)\n",
            "test_steps: 200 0.0 (128,) (128,)\n",
            "test_steps: 300 0.0078125 (128,) (128,)\n",
            "test_steps: 400 0.0390625 (128,) (128,)\n",
            "test_steps: 500 0.234375 (128,) (128,)\n",
            "test_steps: 600 0.4609375 (128,) (128,)\n",
            "test_steps: 700 0.6640625 (128,) (128,)\n",
            "test_steps: 800 0.84375 (128,) (128,)\n",
            "test_steps: 900 0.921875 (128,) (128,)\n",
            "test_steps: 1000 0.9609375 (128,) (128,)\n",
            "test_steps: 1100 0.984375 (128,) (128,)\n",
            "TESTING: Average return: 0.16608466683512355, Average loss: 4.711589813232422, Average q value-estimation: -0.22868523001670837\n",
            "   average_return  average_loss  average_q_values  step\n",
            "0        0.166085       4.71159         -0.228685     0\n",
            "Training step: 1\n",
            "Training step: 2\n",
            "Training step: 3\n",
            "Training step: 4\n",
            "Training step: 5\n",
            "Training step: 6\n",
            "Training step: 7\n",
            "Training step: 8\n",
            "Training step: 9\n",
            "Training step: 10\n",
            "Training step: 11\n",
            "Training step: 12\n",
            "Training step: 13\n",
            "Training step: 14\n",
            "Training step: 15\n",
            "Training step: 16\n",
            "Training step: 17\n",
            "Training step: 18\n",
            "Training step: 19\n",
            "Training step: 20\n",
            "Training step: 21\n",
            "Training step: 22\n",
            "Training step: 23\n",
            "Training step: 24\n",
            "Training step: 25\n",
            "Training step: 26\n",
            "Training step: 27\n",
            "Training step: 28\n",
            "Training step: 29\n",
            "Training step: 30\n",
            "Training step: 31\n",
            "Training step: 32\n",
            "Training step: 33\n",
            "Training step: 34\n",
            "Training step: 35\n",
            "Training step: 36\n",
            "Training step: 37\n",
            "Training step: 38\n",
            "Training step: 39\n",
            "Training step: 40\n",
            "Training step: 41\n",
            "Training step: 42\n",
            "Training step: 43\n",
            "Training step: 44\n",
            "Training step: 45\n",
            "Training step: 46\n",
            "Training step: 47\n",
            "Training step: 48\n",
            "Training step: 49\n",
            "Training step: 50\n",
            "Training step: 51\n",
            "Training step: 52\n",
            "Training step: 53\n",
            "Training step: 54\n",
            "Training step: 55\n",
            "Training step: 56\n",
            "Training step: 57\n",
            "Training step: 58\n",
            "Training step: 59\n",
            "Training step: 60\n",
            "Training step: 61\n",
            "Training step: 62\n",
            "Training step: 63\n",
            "Training step: 64\n",
            "Training step: 65\n",
            "Training step: 66\n",
            "Training step: 67\n",
            "Training step: 68\n",
            "Training step: 69\n",
            "Training step: 70\n",
            "Training step: 71\n",
            "Training step: 72\n",
            "Training step: 73\n",
            "Training step: 74\n",
            "Training step: 75\n",
            "Training step: 76\n",
            "Training step: 77\n",
            "Training step: 78\n",
            "Training step: 79\n",
            "Training step: 80\n",
            "Training step: 81\n",
            "Training step: 82\n",
            "Training step: 83\n",
            "Training step: 84\n",
            "Training step: 85\n",
            "Training step: 86\n",
            "Training step: 87\n",
            "Training step: 88\n",
            "Training step: 89\n",
            "Training step: 90\n",
            "Training step: 91\n",
            "Training step: 92\n",
            "Training step: 93\n",
            "Training step: 94\n",
            "Training step: 95\n",
            "Training step: 96\n",
            "Training step: 97\n",
            "Training step: 98\n",
            "Training step: 99\n",
            "Training step: 100\n",
            "Training step: 101\n",
            "Training step: 102\n",
            "Training step: 103\n",
            "Training step: 104\n",
            "Training step: 105\n",
            "Training step: 106\n",
            "Training step: 107\n",
            "Training step: 108\n",
            "Training step: 109\n",
            "Training step: 110\n",
            "Training step: 111\n",
            "Training step: 112\n",
            "Training step: 113\n"
          ]
        }
      ],
      "source": [
        "dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddYR9uUSU1OX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}