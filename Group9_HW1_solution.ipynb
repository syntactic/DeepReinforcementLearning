{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Understanding MDPs\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.1 Chess\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 LunarLander\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Model Based RL: Accessing Environment Dynamics\n",
    "------------\n",
    "\n",
    "The environment dynamics are controlled by the *reward function* and the *state transition function*. The *reward function* determines the reward that an action will produce if taken in the current state. The *state transition function* takes in a proposed action and the current state and returns the state that would be produced by taking this action. In a very simple implementation, a positive reward will be returned if the action produces a 'good' state and a negative reward, or cost, will be returned if the action produces a 'bad' state, as returned by the state transition function. For example, imagine that you have an actor who is tasked with caring for a plant and can either water or not water the plant once a day. The state transition function takes in the plant's current state, it's thirst level, and a proposed action, either watering or not watering the plant, and returns the thirst level of the plant after each action. The reward function can take in these proposed states, and return a reward based on whether the plant is at a health thirst level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Implementing a Gridworld\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Look up some examples\n",
    "----------\n",
    "\n",
    "1. [An introduction article that describes a simple implementation](https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff)\n",
    "2. [A simple visualization of deterministic and probabilitic policies for a gridworld with obstacles](https://www.youtube.com/watch?v=gThGerajccM)\n",
    "3. [A simple toy gridworld with explanation](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Implementing the MDP\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# Directions as an enum\n",
    "class Direction(Enum):\n",
    "    UP = 0\n",
    "    LEFT = 1\n",
    "    DOWN = 2\n",
    "    RIGHT = 3\n",
    "    \n",
    "def direction_arithmetic(curr_pos, direction):\n",
    "    row, col = curr_pos\n",
    "    if direction == Direction.UP:\n",
    "        row = row - 1\n",
    "    elif direction == Direction.LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == Direction.DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == Direction.RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return (row, col)\n",
    "\n",
    "# Define the GridWorld Class\n",
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, width, height):\n",
    "        # initialize the grid properties - these may never change\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = [[0 for _ in range(width)] for _ in range(height)]\n",
    "        self.win_state = (width-1,height-1)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    # for resetting the state of the world to the beginning\n",
    "    def reset(self):\n",
    "        for i in range(len(self.grid)):\n",
    "            for j in range(len(self.grid[i])):\n",
    "                self.grid[i][j] = 0\n",
    "        self.agent_state = (0,0)\n",
    "        self.grid[self.agent_state[0]][self.agent_state[1]] = \"A\"\n",
    "        self.grid[self.win_state[0]][self[] = \"W\"\n",
    "    \n",
    "    def valid(self, state):\n",
    "        row, col = state\n",
    "        # the below condition would need to be updated for obstacles\n",
    "        return (row >=0 and row < self.height) and (col >=0 and col < self.width)\n",
    "        \n",
    "    def move(self, direction):\n",
    "        target_state = direction_arithmetic(self.agent_state, direction)\n",
    "        if self.valid(target_state):\n",
    "            self.grid[self.agent_state[0]][self.agent_state[1]] = 0\n",
    "            self.agent_state = target_state\n",
    "            self.grid[self.agent_state[0]][self.agent_state[1]] = \"A\"\n",
    "    \n",
    "    # for printing and debugging\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mGridWorld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[1;32m      3\u001b[0m g\u001b[38;5;241m.\u001b[39mmove(Direction\u001b[38;5;241m.\u001b[39mLEFT)\n",
      "Cell \u001b[0;32mIn[47], line 33\u001b[0m, in \u001b[0;36mGridWorld.__init__\u001b[0;34m(self, width, height)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(width)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(height)]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwin_state \u001b[38;5;241m=\u001b[39m (width\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,height\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 42\u001b[0m, in \u001b[0;36mGridWorld.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_state \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_state[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_state[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid[\u001b[43mwidth\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][height\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'width' is not defined"
     ]
    }
   ],
   "source": [
    "g = GridWorld(3,3)\n",
    "print(g)\n",
    "g.move(Direction.LEFT)\n",
    "print(g)\n",
    "g.move(Direction.RIGHT)\n",
    "print(g)\n",
    "g.move(Direction.DOWN)\n",
    "print(g)\n",
    "g.reset()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Implementing a policy\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Implement the basic agent\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Evaluate the policy\n",
    "----------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4eeb274f15e3ae30383e5c78d3f581e2b5f9110a6ec26e1c0da3a4308d4aee77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
