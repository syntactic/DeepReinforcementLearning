{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1 Understanding MDPs\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.1 Chess\n",
    "------------\n",
    "\n",
    "In the following we will try to provide explanations and formalizations for the game chess as we know it, but for deeper understanding and better representations we will also provide a \"simpler\" chess version in MDP. The simpler chess version has following rules:\n",
    "\n",
    "- The simple chess game is played on a 4x4 board\n",
    "- There are only two pawns, a black and a white one\n",
    "- Pawns can move one square forward or one square diagonally forward\n",
    "- Capturing is not possible\n",
    "- Two pawns cannot be on the same square\n",
    "- The pawn that reaches the opposite side of the board wins and the game ends\n",
    "\n",
    "**State(S}**: The set of possible configurations of the chess board (8x8) and the pieces (king, queen, rook, knight, bishop, pawn) at a certain time. At the beginning of the game, the initial state might be: S₀ - where the Board is at its initial configuration with the pieces arranged in their starting positions. Each square of the 8x8 board can have certain properties like occupation (occupied or empty) and color (white or black). Another important variable is the turn of the player (player 1 or player 2 turn). \n",
    "\n",
    "simpler chess version: The state s = (xw, yw, xb, yb, t) x and y represent the positions of the white (w) and black (b) pawns on the board and the turn t the turn of the player (e.g. 1 for white's turn, 0 for black)\n",
    "\n",
    "**State space**: Number of possible states would be very large - one has to consider that a 8x8 board is being used for chess. But counting in all of the possible moves of all of the 32 pieces is difficult, since not all of the pieces can move (in the same manner) each turn and the number of possible moves increases with the game-progress. The number of possible state spaces is therefore estimated to be around 10^47- where the not legal starting configurations are included as well.\n",
    "\n",
    "simpler chess version: In our simple chess version the total number of state spaces would be the number of possible positions of the white pawn (16) multiplied by the number of possible positions of the black pawn (16) multiplied by the number of possible turns (2). In this case the number of state space would be 16 x 16 x 2 = 512. If we even add a fixed starting position, allowing the pawns starting only at one square of the opposite rows of the board, the possible squares where the pawns will start the game will be 4 each and therefore the total number of state spaces in that case would reduce to 4 x 4 x 2 = 32.\n",
    "\n",
    "State space S = {(xw, yw, xb, yb, t) | xw, xw, yb, yb ∈ {1,2,3,4}, t ∈ {0,1}}\n",
    "\n",
    "\n",
    "**Action(A)**: The actions should describe the possible movements of the player regarding the current state. In the case of chess, that contains all possible movements of all the pieces (king, queen, rook, knight, bishop, pawn) at every state in the game. Movements can be represented by a pair of positions on the board - the starting square and the ending square position of the played piece. Changes like capturing the opponent's piece or special rules, are to be included. Depending on the state and turn, the specific agent (e.g. knight) can be moved from the represented position of b1 to c3.\n",
    "\n",
    "A = (b1,c3)\n",
    "\n",
    "simpler chess version: Our Pawns can move either forward, diagonally right or diagonally left\n",
    "\n",
    "A = {move forward, move forward-right, move forward-left}\n",
    "\n",
    "**Reward**: A Reward of +100 could be given to the player, that is the one to checkmate the opponent, conversely a reward of -100 can be given to the player that is being checkmated. In all the other cases (game is still going on, draw) a reward of 0 can be given. Losing pieces from being captured that are important for preventing the king from being checkmated by the opponent should be integrated to the costs of the reward function as well. Therefore, capturing the queen of the opponent might also return a positive reward +1 and losing a queen in the game a negative one -1. \n",
    "\n",
    "simpler chess version: There is a reward of +1 for the pawn, that reaches the end of the board, while the opposite pawn will get a reward of -1. When there is a draw in the terminal state, a reward of 0 is given to both. Otherwise a reward of -0.1 can be given, to motivate the players to reach the goal.\n",
    "\n",
    "For the black pawn, the reward function would look like this:\n",
    "\n",
    "$R(s,a,s') = $\n",
    "$\n",
    "\\begin{cases}\n",
    "    1,      & \\text{if } s' \\text{ the black pawn is on the first row} \\\\\\\\\n",
    "    -1,      & \\text{if } s' \\text{ the white pawn is on the fourth row} \\\\\\\\\n",
    "    -0.1,      & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "For the white pawn, it would be the other way around.\n",
    "\n",
    "Example: Let's say the game is at the state s = (1, 2, 2, 3, 0) - where (1,2) represents the position of white pawn, (2,3) the position of the black pawn and the 0 the turn of the black pawn. Then follows the new state s' = (1,2,3,3,1) - where the black pawn has moved and it is white's turn. The reward for this move is -0.1. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Transition Probabilities**: The transition probability is depending on the current state and the actions taken until now and the opponent’s potential responses. For *p(s′|s, a)*, the state s′ expresses the state after a changed position in the game (e.g. a pawn moved from position f2 to f3). The probability depends on the opponent’s response (multiple options) which again leads to different states with different probabilities. \n",
    "\n",
    "**Policy**: The policy describes the optimal action to achieve the goal when at a certain state. So for a state where the option is to checkmate the opponent with the next move or only in 3 moves, the first move is the optimal action to achieve the goal and the probability should be much higher.\n",
    "\n",
    "$\\pi(a|s) = \\begin{cases} 1 & \\text{if } a = \\operatorname*{argmax}{a'} \\sum{s',r} p(s',r|s,a') r \\ 0 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "In this expression, $\\sum_{s',r} p(s',r|s,a')r$ represents the expected reward of taking action $a'$ in state $s$, and $\\operatorname*{argmax}_{a'}$ denotes the action that maximizes this expected reward. The policy selects the action that maximizes the expected reward for the given state $s$, and assigns a probability of 1 to that action, and a probability of 0 to all other actions.\n",
    "\n",
    "simpler chess version: An example for a policy of the white pawn in our simplified chess game may look like:\n",
    "\n",
    "$\\pi(s) = \\begin{cases}\n",
    "    \\text{move forward until reaching the fourth row} & \\text{if the square is not occupied} \\\\\n",
    "  \\text{move diagonally forward-right if possible,} \\\n",
    "\\text{otherwise move diagonally forward-left} & \\text{if square is occupied} \\\n",
    "\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 LunarLander\n",
    "------------\n",
    "[LunarLander source we used](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In the MDP of LunarLander, the actor is the shuttle and the state is a vector representing horizontal and vertical components of the shuttle's position ($p$), linear speed ($v$), and angular speed ($\\theta$) of the shuttle, as well as a boolean of whether the right and/or left leg of the shuttle is touching the ground: \n",
    "\n",
    "$|S| = [p_{x}, p_{y}, v_{x}, v_{y}, \\theta_{x}, \\theta_{y}, RIGHT\\_LEG, LEFT\\_LEG]$\n",
    "\n",
    "The state space contains all possible positions, linear velocities, angular velocities, and combinations of right and left leg ground contact that are possible, though these limits are not explictly defined in the source. \n",
    "\n",
    "The actor has four possible actions, which all regard the use of the engines:\n",
    "\n",
    "A = \\{do nothing, fire left engine, fire main engine, fire right engine\\}\n",
    "\n",
    "In order to achieve the win state, the actor must accumulate 200 points. The source we used ([updated link](https://gymnasium.farama.org/environments/box2d/lunar_lander/)) did not specify how much the reward should increase or decrease based on change in position, linear velocity, or angular velocy, only the direction that it should change. We created our own reward function to reflect the rewards described, but it may not be optimal. \n",
    "\n",
    "$R_{a}(s, s') =$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $(p_{x} - p'_{x} + p_{y} - p'_{y}) +$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $(v_{x} - v'_{x} + v_{y} - v'_{y})+$  \\\n",
    "    &nbsp;&nbsp;&nbsp; $(\\theta_{x} - \\theta'_{x} + \\theta_{y} - \\theta'_{y}) + $ \\\n",
    "    &nbsp;&nbsp;&nbsp; $10*RIGHT\\_LEG' + 10*LEFT\\_LEG' +$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $\\text{if}(a == 1 \\text{ or } a == 3)\\{-0.03\\} +$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $\\text{if}(a == 2)\\{-0.3\\} +$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $\\text{if}(RIGHT\\_LEG' \\text{ and } LEFT\\_LEG')\\{100\\} +$ \\\n",
    "    &nbsp;&nbsp;&nbsp; $\\text{if}(!RIGHT\\_LEG' \\text{ and } !LEFT\\_LEG' \\text{ and } p'_{y} \\leq 0)\\{-100\\}$\n",
    "    \n",
    "The policy describes the optimal action to take in a given state. In the case of LunarLander, the optimal action leads to the goal of landing, without crashing, on the landing pad. In a practical description, if the angular speed is positive, the policy would return 'fire right engine', or if the angular speed is negative, the policy would return 'fire left engine', to straighten the shuttle. Similarly, if the shuttle's linear speed is too fast to allow time for straightening before collision, the 'fire main engine' action would be optimal to increase the vertical distance between the shuttle and ground. The reward function may also take into account how far, and in which direction, the shuttle is offset from the center of the landing pad, and reward the left or right engine action to change the shuttle's course. If the shuttle is straight and above the landing pad, the 'do nothing' action would be optimal, since gravity will carry the shuttle to the ground. \n",
    "\n",
    "Below is a simple policy. The term $\\sqrt{\\theta_{x}^2 + \\theta_{y}^2}$ computes the overall magnitude of shuttle angular velocity, while $\\tan^{-1}(theta_{y}*theta_{x})$ computes the direction of angular velocity (when positive it is counter-clockwise, and when negative it is clockwise). If the shuttle is above the landing pad and approximately level, $\\pi(\\text{'do nothing'}|s) = 1$. If the shuttle is high enough above the ground to have time to straighten ($p_{y} \\geq 5$) and is turning counter clockwise, $\\pi(\\text{'fire left engine'}|s) = 1$. Similarly, if the same conditions are met but the shuttle is turning clockwise, $\\pi(\\text{'fire right engine'}|s) = 1$. If the shuttle is not level and above the landing pad, but is too close to the ground to correct, $\\pi(\\text{'fire main engine'}|s) = 1$. The policy statements should be evaluated in the order we present here. We used a heuristic of distance from the $y=0$ instead of considering the shuttle's linear velocity to simplify the policy, but this is not optimal.\n",
    "\n",
    "$\\pi(\\text{'do nothing'}|s) =\n",
    "$\n",
    "$\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } |p_{x} < 5| \\text{ and } \\sqrt{\\theta_{x}^2 + \\theta_{y}^2} < 5\\\\\n",
    "    0 & \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "$\\pi(\\text{'fire left engine'}|s) =\n",
    "$\n",
    "$\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\tan^{-1}(theta_{y}*theta_{x}) > 0 \\text{ and } \\sqrt{\\theta_{x}^2 + \\theta_{y}^2} \\geq 5 \\text{ and } p_{y} > 5\\\\\n",
    "    0 & \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "$\\pi(\\text{'fire right engine'}|s) =\n",
    "$\n",
    "$\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\tan^{-1}(theta_{y}*theta_{x}) \\leq 0 \\text{ and } \\sqrt{\\theta_{x}^2 + \\theta_{y}^2} \\geq 5 \\text{ and } p_{y} > 5\\\\\n",
    "    0 & \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "$\\pi(\\text{'fire main engine'}|s) =\n",
    "$\n",
    "$\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\sqrt{\\theta_{x}^2 + \\theta_{y}^2} \\geq 5 \\text{ and } p_{y} \\leq 5\\\\\n",
    "    0 & \\text{ otherwise }\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.3 Model Based RL: Accessing Environment Dynamics\n",
    "------------\n",
    "\n",
    "The environment dynamics are controlled by the *reward function* and the *state transition function*. The *reward function* determines the reward that an action will produce if taken in the current state. The *state transition function* takes in a proposed action and the current state and returns the state that would be produced by taking this action. In a very simple implementation, a positive reward will be returned if the action produces a 'good' state and a negative reward, or cost, will be returned if the action produces a 'bad' state, as returned by the state transition function. For example, imagine that you have an actor who is tasked with caring for a plant and can either water or not water the plant once a day. The state transition function takes in the plant's current thirst level (state), and a proposed action, either watering or not watering the plant, and returns the thirst level of the plant after each action. The reward function can take in these proposed states, and return a reward based on whether the plant is at a healthy thirst level after taking a specific action. Environment dynamics, $p(s',r|s,a)$, capture this relationship. Another example is the computer game *Pong* where the goal for the actor is to score eleven points against the opponent by hitting the ball off of the opponent side of the screen. In this case, the states contain information about the position of the actor and opponent, and the position and trajectory of the ball. The *state transition function* returns, in part, the expected position of the opponent and the trajectory of the ball after taking a proposed action. The *reward function* may return a larger reward based on the difference between the trajectory of the ball and the angle between the ball and the opponent, for example.\n",
    "\n",
    "If environment dynamics are known, this relationship between the *reward function* and *state transisiton function* produces reenforcement that supports the goal of the actor. However, in practical applications, environment dynamics are often uncontrolled and difficult to represent efficiently in data. For example, in the case of self driving cars, one goal may be to arrive at the destination quickly and safetly. Approaching an intersection, the environment dynamics modeled by our formulation of the problem may reward the action of acceleration, however, unanticipated factors may impact the next state that the *state transition function* does not capture. A pedestrian may suddenly cross into the road, or the car ahead of the actor may slow down, such that the new state does not reflect the state predicted by the *state transition function*, and acceleration actually produces an undesired result. In this case, the actor acts within an environment where other agents have unknown behaviors that change the environment. In some cases, the information necessary to perform tasks may be recoverable from the environment, but still be too complicated to compute and store efficiently. Continuing our example, the representation of the state could include estimations of the uncontrolled agents' next actions, for instance the behavior of the pedestrians or the goal direction of other drivers, but with many unknown variables these estimations will probably be inaccurate and require a large amount of data. Often the goals of the actor can produce conflicting answers as well, such as reaching a destination quickly *and* safetly. Together, unanticipated factors and complex factors make it impossible to perfectly capture environment dynamics in real world situations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Implementing a Gridworld\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Look up some examples\n",
    "----------\n",
    "\n",
    "1. [An introduction article that describes a simple implementation](https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff)\n",
    "2. [A simple visualization of deterministic and probabilitic policies for a gridworld with obstacles](https://www.youtube.com/watch?v=gThGerajccM)\n",
    "3. [A simple toy gridworld with explanation](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Implementing the MDP\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MDP has a variable number of states, depending on how the grid world is initialized. Maybe we want it to be fixed? There are two kinds of complexities - walls and warps. Implicitly there are walls at the edges of the world, and there are also randomly placed walls within the grid which do not permit movement to the tile the wall is sitting on. Warps are tiles which, when moved to, send the agent back to the beginning position.\n",
    "\n",
    "$|S|=m*n$\n",
    "\n",
    "$A=\\{up, left, down, right\\}$\n",
    "\n",
    "$P_{a}(s, s') =\n",
    "$$\n",
    "\\begin{cases}\n",
    "    1,      & \\text{if } s \\text{ isn't a wall or warp} \\\\\\\\\n",
    "    0,      & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$R_{a}(s, s') = $\n",
    "$\n",
    "\\begin{cases}\n",
    "    1,      & \\text{if } s' \\text{ equals win state } \\\\\\\\\n",
    "    -0.5,      & \\text{if } s' \\text{ equals warp } \\\\\\\\\n",
    "    -0.25,      & \\text{if } s' \\text{ equals wall } \\\\\\\\\n",
    "    0,      & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "This MDP is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "\n",
    "\n",
    "def distance_between_states(state_1, state_2):\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2[0] - state_1[0]) + abs(state_2[1] - state_1[1])\n",
    "    \n",
    "def direction_arithmetic(curr_pos, direction):\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return (row, col)\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[0 for _ in range(width)] for _ in range(height)]\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return (rand_row, rand_col)\n",
    "    \n",
    "    def tile_is_open(self, tile):\n",
    "        return self.grid[tile[0]][tile[1]] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        tile = self.random_position()\n",
    "        if (self.tile_is_open(tile) and\n",
    "            distance_between_states(tile, self.win_state) > 1 and\n",
    "            distance_between_states(tile, self.agent_state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(tile)\n",
    "                self.grid[tile[0]][tile[1]] = \"█\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(tile)\n",
    "                self.grid[tile[0]][tile[1]] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        for i in range(len(self.grid)):\n",
    "            for j in range(len(self.grid[i])):\n",
    "                self.grid[i][j] = \" \"\n",
    "        self.agent_state = (0, 0)\n",
    "        self.win_state = (self.height-1, self.width-1)\n",
    "        self.grid[self.agent_state[0]][self.agent_state[1]] = \"A\"\n",
    "        self.grid[self.win_state[0]][self.win_state[1]] = \"W\"\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < 2:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < 2:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "                \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        row, col = state\n",
    "        return (row >=0 and row < self.height) and (col >=0 and col < self.width)\n",
    "    \n",
    "    def get_valid_next_actions_and_states(self, state=None):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        if state is None:\n",
    "            state = self.agent_state\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        return valid_actions, valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent_state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent_state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent_state[0]][self.agent_state[1]] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent_state = (0, 0)\n",
    "            else:\n",
    "                self.agent_state = target_state\n",
    "            self.grid[self.agent_state[0]][self.agent_state[1]] = \"A\"\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "|A| | | | |\n",
      "===========\n",
      "| | |*| |█|\n",
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | | |█| |\n",
      "===========\n",
      "| | |*| |W|\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "g = GridWorld(5,5, True)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Implementing a policy\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Implement the basic agent\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Evaluate the policy\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "|A| | | | |\n",
      "===========\n",
      "| | |*| |█|\n",
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | | |█| |\n",
      "===========\n",
      "| | |*| |W|\n",
      "===========\n",
      "-1.111\t-0.908\t-1.066\t-1.035\t-1.267\t\n",
      "-0.820\t-0.951\t0.000\t-0.928\t0.000\t\n",
      "-0.808\t-0.857\t-0.877\t-0.472\t0.259\t\n",
      "-0.857\t-0.962\t-1.140\t0.000\t0.750\t\n",
      "-0.930\t-1.069\t0.000\t0.000\t0.000\t\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95\n",
    "completed_episodes = 0\n",
    "HEIGHT = 5\n",
    "WIDTH = 5\n",
    "V = [[0 for _ in range(WIDTH)] for _ in range(HEIGHT)]\n",
    "returns = [[ [] for _ in range(WIDTH)] for _ in range(HEIGHT)]\n",
    "g = GridWorld(HEIGHT, WIDTH, complex=True)\n",
    "print(g)\n",
    "\n",
    "def average(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()\n",
    "        \n",
    "while completed_episodes < 1000:\n",
    "    time_step = 0\n",
    "    visited_states = list()\n",
    "    \n",
    "    \"\"\"the agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "    while g.agent_state != g.win_state:\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        possible_next_actions, possible_next_states = g.get_valid_next_actions_and_states()\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, g.win_state), possible_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        selected_action = random.choices(possible_next_actions, weights=reciprocals_of_distances)[0]\n",
    "        \n",
    "        \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "           that have been visited so far this episode.\"\"\"\n",
    "        reward_from_action = g.reward(selected_action)\n",
    "        for i in range(len(visited_states)):\n",
    "            state_in_history = visited_states[-1*i] # moving backwards in time\n",
    "            state_in_history[1] += gamma**i * reward_from_action # element 1 is the reward\n",
    "        visited_states.append([g.agent_state, reward_from_action])\n",
    "        \n",
    "        \"\"\"Make the move and increase the time step.\"\"\"\n",
    "        g.move(selected_action)\n",
    "        time_step += 1\n",
    "        \n",
    "    \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "       by (row, col)). Then recalculate V based on the ever growing returns lists. As they grow, the\n",
    "       values in V should converge.\"\"\"\n",
    "    for visited_state in visited_states:\n",
    "        state = visited_state[0]\n",
    "        rewards = visited_state[1]\n",
    "        returns[state[0]][state[1]].append(rewards)\n",
    "        V[state[0]][state[1]] = average(returns[state[0]][state[1]])\n",
    "    completed_episodes += 1\n",
    "    \n",
    "    g.reset()\n",
    "\n",
    "print_matrix(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "4eeb274f15e3ae30383e5c78d3f581e2b5f9110a6ec26e1c0da3a4308d4aee77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
