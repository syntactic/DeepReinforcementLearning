{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c61bcd-cb9d-49a7-9b33-04350322ca8f",
   "metadata": {},
   "source": [
    "## 2 Learning a policy via MC - Policy Iteration\n",
    "For the following work with last weeks implementation of your own gridworld! You may revise/change pieces of it, or ask other groups for access to their implementation of course.\n",
    "\n",
    "* Implement tabular MC-estimate Policy Iteration\n",
    "* Measure average Return-per-Episode and plot it against (1) episodes sam-\n",
    "pled, and (2) wallclock-time For an outstanding submission:\n",
    "* Visualize the State-Action Values in your gridworld during training at regular intervals, and provide a visualization of them (e.g. a series of images, best combine them into a short video clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ba086-e4db-4861-b0ff-1c78492c6a2f",
   "metadata": {},
   "source": [
    "### GridWorld Code (refactored) From HW 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c83dc-f304-438b-8f36-c84612da442b",
   "metadata": {},
   "source": [
    "#### Global Variables, Static Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301428f4-3992-4f48-aa9e-8833ce18ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "\n",
    "def distance_between_states(state_1, state_2):\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2[0] - state_1[0]) + abs(state_2[1] - state_1[1])\n",
    "    \n",
    "def direction_arithmetic(curr_pos, direction):\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return (row, col)\n",
    "\n",
    "def average(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240d20f-c2c2-4db5-9f4c-3edb889236eb",
   "metadata": {},
   "source": [
    "#### Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cfc510-26e4-4c28-9987-8c59c2e0c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"This base agent class just takes random actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state=(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "    def get_action_from_policy(self):\n",
    "        return random.choice(self.available_actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        action_index = self.available_action.index(action)\n",
    "        self.state = self.available_next_states[action_index]\n",
    "        \n",
    "    def reset(self, state=(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "class MagneticAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=(0, 0), start_state=(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self):\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, self.win_state), self.available_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        return random.choices(self.available_actions, weights=reciprocals_of_distances)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50946bd-d39e-4808-87bd-4540ecf1fbfe",
   "metadata": {},
   "source": [
    "#### GridWorld Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ae41ba-83c3-4067-add5-908fe288e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[0 for _ in range(width)] for _ in range(height)]\n",
    "        self.agent = Agent()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return (rand_row, rand_col)\n",
    "    \n",
    "    def tile_is_open(self, tile):\n",
    "        return self.grid[tile[0]][tile[1]] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        tile = self.random_position()\n",
    "        if (self.tile_is_open(tile) and\n",
    "            distance_between_states(tile, self.win_state) > 1 and\n",
    "            distance_between_states(tile, self.agent.state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(tile)\n",
    "                self.grid[tile[0]][tile[1]] = \"â–ˆ\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(tile)\n",
    "                self.grid[tile[0]][tile[1]] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        for i in range(len(self.grid)):\n",
    "            for j in range(len(self.grid[i])):\n",
    "                self.grid[i][j] = \" \"\n",
    "        self.win_state = (self.height-1, self.width-1)\n",
    "        self.grid[self.win_state[0]][self.win_state[1]] = \"W\"\n",
    "        \n",
    "        self.agent.reset()\n",
    "        self.grid[self.agent.state[0]][self.agent.state[1]] = \"A\"\n",
    "        self.update_valid_next_actions_and_states()\n",
    "\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < 2:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < 2:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "                \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        row, col = state\n",
    "        return (row >=0 and row < self.height) and (col >=0 and col < self.width)\n",
    "    \n",
    "    def update_valid_next_actions_and_states(self):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(self.agent.state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        self.agent.available_actions = valid_actions\n",
    "        self.agent.available_next_states = valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent.state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent.state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent.state[0]][self.agent.state[1]] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent.state = (0, 0)\n",
    "            else:\n",
    "                self.agent.state = target_state\n",
    "            self.grid[self.agent.state[0]][self.agent.state[1]] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f2b58-d0a4-4acf-97f6-adbb5795e23c",
   "metadata": {},
   "source": [
    "### Run Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0627e44e-bbae-4c53-8b63-60bc7c8e05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GAMMA = 0.95\\nHEIGHT = 5\\nWIDTH = 5\\nV = [[0 for _ in range(WIDTH)] for _ in range(HEIGHT)]\\nreturns = [[ [] for _ in range(WIDTH)] for _ in range(HEIGHT)]\\ng = GridWorld(HEIGHT, WIDTH, complex=True)\\na = MagneticAgent()\\ng.set_agent(a)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"GAMMA = 0.95\n",
    "HEIGHT = 5\n",
    "WIDTH = 5\n",
    "V = [[0 for _ in range(WIDTH)] for _ in range(HEIGHT)]\n",
    "returns = [[ [] for _ in range(WIDTH)] for _ in range(HEIGHT)]\n",
    "g = GridWorld(HEIGHT, WIDTH, complex=True)\n",
    "a = MagneticAgent()\n",
    "g.set_agent(a)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942196f5-411c-4f37-93c4-63cf44e47d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(height, width, gamma, agent, num_episodes):\n",
    "    V = [[0 for _ in range(width)] for _ in range(height)]\n",
    "    returns = [[ [] for _ in range(width)] for _ in range(height)]\n",
    "    g = GridWorld(height, width, complex=True)\n",
    "    g.set_agent(agent)\n",
    "    \n",
    "    completed_episodes = 0\n",
    "    while completed_episodes < 500:\n",
    "        time_step = 0\n",
    "        visited_states = list()\n",
    "        g.reset()\n",
    "\n",
    "        \"\"\"the agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while g.agent.state != g.win_state:\n",
    "\n",
    "            selected_action = g.agent.get_action_from_policy()\n",
    "\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            reward_from_action = g.reward(selected_action)\n",
    "            for i in range(len(visited_states)):\n",
    "                state_in_history = visited_states[-1*i] # moving backwards in time\n",
    "                state_in_history[1] += gamma**i * reward_from_action # element 1 is the reward\n",
    "            visited_states.append([g.agent.state, reward_from_action])\n",
    "\n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            g.move(selected_action)\n",
    "            time_step += 1\n",
    "\n",
    "        \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "           by (row, col)). Then recalculate V based on the ever growing returns lists. As they grow, the\n",
    "           values in V should converge.\"\"\"\n",
    "        for visited_state in visited_states:\n",
    "            state = visited_state[0]\n",
    "            rewards = visited_state[1]\n",
    "            returns[state[0]][state[1]].append(rewards)\n",
    "            V[state[0]][state[1]] = average(returns[state[0]][state[1]])\n",
    "        completed_episodes += 1\n",
    "\n",
    "        if completed_episodes % 10 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32820e0-54a7-45d6-87f3-e8acbe4d0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V = monte_carlo_simulation(5, 5, 0.95, Agent(), 500)\n",
    "#print_matrix(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a868a-a125-47e4-9b03-97a2abc4b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episodes: 10\n",
      "Completed episodes: 20\n",
      "Completed episodes: 30\n",
      "Completed episodes: 40\n",
      "Completed episodes: 50\n",
      "Completed episodes: 60\n",
      "Completed episodes: 70\n",
      "Completed episodes: 80\n",
      "Completed episodes: 90\n",
      "Completed episodes: 100\n",
      "Completed episodes: 110\n",
      "Completed episodes: 120\n",
      "Completed episodes: 130\n",
      "Completed episodes: 140\n",
      "Completed episodes: 150\n",
      "Completed episodes: 160\n",
      "Completed episodes: 170\n",
      "Completed episodes: 180\n",
      "Completed episodes: 190\n",
      "Completed episodes: 200\n",
      "Completed episodes: 210\n"
     ]
    }
   ],
   "source": [
    "V = monte_carlo_simulation(5, 5, 0.95, MagneticAgent(), 500)\n",
    "print_matrix(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
