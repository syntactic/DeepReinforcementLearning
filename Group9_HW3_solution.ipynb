{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e0042a",
   "metadata": {},
   "source": [
    "# DRL Homework 03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4d76a4",
   "metadata": {},
   "source": [
    "### 1 Homework Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a91ad9",
   "metadata": {},
   "source": [
    "### 2 Learning a policy via 1-step SARSA\n",
    "For the following work again work with your own gridworld implementation! You may revise/change pieces of it, or ask other groups for access to their implementation of course.\n",
    "* Implement tabular 1-step SARSA control\n",
    "* Measure average Return-per-Episode and plot it against (1) episodes sampled, and (2) wallclock-time\n",
    "\n",
    "For an outstanding submission:\n",
    "\n",
    "* Visualize the State-Action Values in your gridworld during training at regular intervals, and provide a visualization of them (e.g. a series of images, best combine them into a short video clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c98ebc3-a5dc-46c8-87eb-82bb5add19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "DIRECTION_TO_SYMBOL = {UP:\"↑\", LEFT:\"←\", DOWN:\"↓\", RIGHT:\"→\", \"EMPTY\":\"█\"}\n",
    "\n",
    "def distance_between_states(state_1: State, state_2: State) -> int:\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2.row - state_1.row) + abs(state_2.col - state_1.col)\n",
    "    \n",
    "def direction_arithmetic(curr_pos: State, direction: int) -> State:\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos.row, curr_pos.col\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return State(row, col)\n",
    "\n",
    "def average(l):\n",
    "    if l:\n",
    "        return sum(l)/len(l)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def argmax(d):\n",
    "    try:\n",
    "        return max(d, key=d.get)\n",
    "    except ValueError:\n",
    "        return \"EMPTY\"\n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()\n",
    "        \n",
    "def print_policy_from_q(q):\n",
    "    s = \"\"\n",
    "    height = len(q)\n",
    "    width = len(q[0])\n",
    "    for row in range(height):\n",
    "        s += \"==\" * (width) + \"=\"\n",
    "        s += \"\\n\"\n",
    "        for col in range(width):\n",
    "            s += f\"|{DIRECTION_TO_SYMBOL[argmax(q[row][col])]}\"\n",
    "        s +=\"|\\n\"\n",
    "    s += \"==\" * (width) + \"=\"\n",
    "    print(s)\n",
    "\n",
    "def max_q(Q):\n",
    "    \"\"\" return a widthxheight array of the maximum returns for each gridcell\n",
    "        in Q and a widthxheight array of the best actions \"\"\"\n",
    "    width = len(Q)\n",
    "    height = len(Q[0])\n",
    "    \n",
    "    values = [[ float(\"-inf\") for _ in range(width)] for _ in range(height)] \n",
    "    actions = [[ [] for _ in range(width)] for _ in range(height)] \n",
    "    \n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            cell_returns = Q[x][y]\n",
    "            if cell_returns:\n",
    "                max_action = max(cell_returns, key= lambda x: cell_returns[x])\n",
    "                max_return = cell_returns[max_action]\n",
    "            \n",
    "                values[x][y] = max_return\n",
    "                actions[x][y] = max_action\n",
    "            \n",
    "    return (values, actions)\n",
    "\n",
    "class State:\n",
    "\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.col == other.col\n",
    "    \n",
    "    def __repr(self):\n",
    "        return f\"({self.row}, {self.col})\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7efce3ae",
   "metadata": {},
   "source": [
    "#### Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab2634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"This base agent class just takes random actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "    def get_action_from_policy(self):\n",
    "        return random.choice(self.available_actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        action_index = self.available_action.index(action)\n",
    "        self.state = self.available_next_states[action_index]\n",
    "        \n",
    "    def reset(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "class MagneticAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self):\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, self.win_state), self.available_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        return random.choices(self.available_actions, weights=reciprocals_of_distances)[0]\n",
    "    \n",
    "class ArgMaxAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self, q_state, epsilon):\n",
    "        chance = random.random()\n",
    "        # TIM: I think the conditions need to come in the opposite order if we want to explore\n",
    "        #if not q_state or chance > (1 - epsilon): # if the state has no information about action returns\n",
    "        if chance > (1 - epsilon) or not q_state:\n",
    "            # randomly pick an action\n",
    "            action = random.choice(self.available_actions)\n",
    "        else:\n",
    "            # get the action with the maximum return value\n",
    "            potential_action = argmax(q_state)\n",
    "            if potential_action in self.available_actions:\n",
    "                action = potential_action\n",
    "            else:\n",
    "                action = random.choice(self.available_actions)\n",
    "        return action\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e3595e7",
   "metadata": {},
   "source": [
    "#### GridWorld class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca3f081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False, win_state=None, start_state=None):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[\" \" for _ in range(width)] for _ in range(height)]\n",
    "        self.start_state = State(0,0) # just for initialization\n",
    "       \n",
    "        \"\"\" initialize the win_state as a random position in the grid, or if an argument \n",
    "            if the argument is provided, as the input win_state \"\"\"\n",
    "        \n",
    "        if win_state is None:\n",
    "            self.win_state = self.random_position()\n",
    "        else:\n",
    "            self.win_state = win_state\n",
    "            \n",
    "        self.grid[self.win_state.row][self.win_state.col] = \"W\"\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < 2:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < 2:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "        \n",
    "        \n",
    "    def set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return State(rand_row, rand_col)\n",
    "    \n",
    "    def state_is_open(self, state: State):\n",
    "        return self.grid[state.row][state.col] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        random_state = self.random_position()\n",
    "        if (self.state_is_open(random_state) and\n",
    "            distance_between_states(random_state, self.win_state) > 1 and \n",
    "            distance_between_states(random_state, self.start_state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"█\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset_agent(self, start=State(0,0)):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        if self.state_is_open(start):\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            self.start_state = start\n",
    "            self.agent.reset(start)\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "            \n",
    "        else:\n",
    "            sys.exit('reset_agent failed')\n",
    "           \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        return (state.row >=0 and state.row < self.height) and (state.col >=0 and state.col < self.width)\n",
    "    \n",
    "    def update_valid_next_actions_and_states(self):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(self.agent.state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        self.agent.available_actions = valid_actions\n",
    "        self.agent.available_next_states = valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent.state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent.state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent.state = self.start_state\n",
    "            else:\n",
    "                self.agent.state = target_state\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "    \n",
    "    \n",
    "    def random_start(self):\n",
    "        \"\"\" select a random state/action that is valid \"\"\"\n",
    "        state_valid = False\n",
    "        actions_possible = False\n",
    "    \n",
    "        while not state_valid and not actions_possible:\n",
    "        \n",
    "            \"\"\" select a random position on the grid as a potential starting state and check that it is open\"\"\"\n",
    "            random_state = self.random_position()\n",
    "            state_valid = self.state_is_open(random_state)\n",
    "            \n",
    "            \"\"\" If the state is open, move the agent to the state and check if there are availble moves \"\"\"\n",
    "            if(state_valid):\n",
    "                self.reset_agent(random_state)\n",
    "                action_possible = len(self.agent.available_actions) > 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a76ba88e",
   "metadata": {},
   "source": [
    "#### Approximating Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f727db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q_mc(gridworld, agent, gamma=0.95, num_episodes=500, track=False):\n",
    "    \"\"\"Approximates Q, a gridworld.width by gridworld.height by actions matrix that \n",
    "       contains the average returns from taking each action in each of the possible\n",
    "       grid states across num_episodes \"\"\"\n",
    "    \n",
    "    \"\"\"Initialize Q and returns as matrices that are width * height * actions to store returns\n",
    "       calculated during each episode. For each state/action pair in the returns matrix, a list \n",
    "       of returns for each episode will accumulate. For each state/action pair in the Q matrix,\n",
    "       the average return across episodes will be stored. A list of wallclock_times are stored\n",
    "       to keep track of the time elapsed per episode\"\"\"\n",
    "    Q = [[ {} for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    returns = [[ { } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    episode_returns = []\n",
    "    avg_returns_per_episode = []\n",
    "    wallclock_times = []\n",
    "    \n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        \"\"\"Initialize the time step (t) for the current episode and create lists to store\n",
    "            visited states and taken actions (index matched) \"\"\"\n",
    "        time_step = 0\n",
    "        visited_states_and_taken_actions = list()\n",
    "        \n",
    "        \"\"\"Get a random valid state to place the agent in and select a first action \"\"\"\n",
    "        gridworld.random_start()\n",
    "        selected_action = random.choice(gridworld.agent.available_actions) \n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            \n",
    "            reward_from_action = gridworld.reward(selected_action)\n",
    "\n",
    "            for i in range(len(visited_states_and_taken_actions)):\n",
    "                state_in_history = visited_states_and_taken_actions[-1*i] # moving backwards in time\n",
    "                state_in_history[1] += gamma**i * reward_from_action # element 1 is the cumulative reward\n",
    "                \n",
    "            \"\"\"Store the current state/action pair \"\"\"\n",
    "            visited_states_and_taken_actions.append([(gridworld.agent.state, selected_action), reward_from_action])\n",
    "            \n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            gridworld.move(selected_action)\n",
    "            time_step += 1\n",
    "            \n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            selected_action = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                     [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "        \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "           by (row, col)). Then recalculate Q based on the ever growing returns lists. As they grow, the\n",
    "           values in Q should converge.\"\"\"\n",
    "        \n",
    "        \"\"\"The return of the episode == The return of the initial (s, a) pair that we chose.\n",
    "        I considered summing all returns from the episode but I don't think that's right.\"\"\"\n",
    "        episode_returns.append(visited_states_and_taken_actions[0][1])\n",
    "        avg_returns_per_episode.append(average(episode_returns))\n",
    "        \n",
    "        for i in range(1, len(visited_states_and_taken_actions)):\n",
    "            step_T_minus_i = visited_states_and_taken_actions[-1*i]\n",
    "            visited_state, taken_action = step_T_minus_i[0]\n",
    "            if (visited_state, taken_action) in map(lambda l : l[0], visited_states_and_taken_actions[0:-1*i]):\n",
    "                continue\n",
    "\n",
    "            rewards = step_T_minus_i[1]\n",
    "            if taken_action not in returns[visited_state.row][visited_state.col]:\n",
    "                returns[visited_state.row][visited_state.col][taken_action] = []\n",
    "            returns[visited_state.row][visited_state.col][taken_action].append(rewards)\n",
    "            Q[visited_state.row][visited_state.col][taken_action] = average(returns[visited_state.row][visited_state.col][taken_action])\n",
    "        \n",
    "        \n",
    "        #avg_returns_per_episode.append(average(returns_per_visited_state))\n",
    "        wallclock_times.append(time.time() - start_time)\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "        \n",
    "    return (Q, avg_returns_per_episode, wallclock_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "010a377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q_sarsa(gridworld: GridWorld, agent: Agent, gamma=0.95, alpha=0.8, num_episodes=500, track=False):\n",
    "    Q = [[ { dir: -0.5 for dir in DIRECTIONS } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    Q[gridworld.win_state.row][gridworld.win_state.col] = { dir: 0 for dir in DIRECTIONS }\n",
    "\n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        gridworld.reset_agent()\n",
    "\n",
    "        \"\"\"Set the next selected action \"\"\"\n",
    "        action_t = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                            [gridworld.agent.state.col], epsilon)\n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            state_t = gridworld.agent.state\n",
    "\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            reward_from_action_t = gridworld.reward(action_t)\n",
    "\n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            gridworld.move(action_t)\n",
    "\n",
    "            \"\"\"In case we happen to terminate\"\"\"\n",
    "            if gridworld.agent.state == gridworld.win_state:\n",
    "                break\n",
    "\n",
    "            state_t_plus_one = gridworld.agent.state\n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            action_t_plus_one = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "            Q[state_t.row][state_t.col][action_t] = Q[state_t.row][state_t.col][action_t] + \\\n",
    "                alpha * (reward_from_action_t + \\\n",
    "                         gamma*Q[state_t_plus_one.row][state_t_plus_one.col][action_t_plus_one] - \\\n",
    "                            Q[state_t.row][state_t.col][action_t])\n",
    "            action_t = action_t_plus_one\n",
    "\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "70a25919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | |█| |█|\n",
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | | |W| |\n",
      "===========\n",
      "|*| |*| | |\n",
      "===========\n",
      "Completed episodes: 100\n",
      "Completed episodes: 200\n",
      "Completed episodes: 300\n",
      "Completed episodes: 400\n",
      "Completed episodes: 500\n",
      "Completed episodes: 600\n",
      "Completed episodes: 700\n",
      "Completed episodes: 800\n",
      "Completed episodes: 900\n",
      "Completed episodes: 1000\n",
      "Completed episodes: 1100\n",
      "Completed episodes: 1200\n",
      "Completed episodes: 1300\n",
      "Completed episodes: 1400\n",
      "Completed episodes: 1500\n",
      "Completed episodes: 1600\n",
      "Completed episodes: 1700\n",
      "Completed episodes: 1800\n",
      "Completed episodes: 1900\n",
      "Completed episodes: 2000\n",
      "Completed episodes: 2100\n",
      "Completed episodes: 2200\n",
      "Completed episodes: 2300\n",
      "Completed episodes: 2400\n",
      "Completed episodes: 2500\n",
      "Completed episodes: 2600\n",
      "Completed episodes: 2700\n",
      "Completed episodes: 2800\n",
      "Completed episodes: 2900\n",
      "Completed episodes: 3000\n",
      "Completed episodes: 3100\n",
      "Completed episodes: 3200\n",
      "Completed episodes: 3300\n",
      "Completed episodes: 3400\n",
      "Completed episodes: 3500\n",
      "Completed episodes: 3600\n",
      "Completed episodes: 3700\n",
      "Completed episodes: 3800\n",
      "Completed episodes: 3900\n",
      "Completed episodes: 4000\n",
      "Completed episodes: 4100\n",
      "Completed episodes: 4200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(g) \u001b[39m# print the gridworld\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# here, returns is an average return value over the entire grid for each episode.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# (ex. returns[0] is the avg. return from ep 1, returns[1] is the avg. return from ep 2)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#(Q, returns, times) = approximate_q_mc(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m Q \u001b[39m=\u001b[39m approximate_q_sarsa(g, ArgMaxAgent(), num_episodes\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, track\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[110], line 29\u001b[0m, in \u001b[0;36mapproximate_q_sarsa\u001b[0;34m(gridworld, agent, gamma, alpha, num_episodes, track)\u001b[0m\n\u001b[1;32m     26\u001b[0m reward_from_action_t \u001b[39m=\u001b[39m gridworld\u001b[39m.\u001b[39mreward(action_t)\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make the move and increase the time step.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m gridworld\u001b[39m.\u001b[39;49mmove(action_t)\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"In case we happen to terminate\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m gridworld\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m gridworld\u001b[39m.\u001b[39mwin_state:\n",
      "Cell \u001b[0;32mIn[73], line 115\u001b[0m, in \u001b[0;36mGridWorld.move\u001b[0;34m(self, direction)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmove\u001b[39m(\u001b[39mself\u001b[39m, direction):\n\u001b[1;32m    113\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m       it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     target_state \u001b[39m=\u001b[39m direction_arithmetic(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mstate, direction)\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid(target_state) \u001b[39mand\u001b[39;00m target_state \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalls:\n\u001b[1;32m    117\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mrow][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcol] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[71], line 34\u001b[0m, in \u001b[0;36mdirection_arithmetic\u001b[0;34m(curr_pos, direction)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized direction: \u001b[39m\u001b[39m{\u001b[39;00mdirection\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[39mreturn\u001b[39;00m State(row, col)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Test approximate_q\"\"\"\n",
    "random.seed(0)\n",
    "height = 5\n",
    "width = 5\n",
    "num_episodes = 5000\n",
    "g = GridWorld(height, width, complex=True)\n",
    "print(g) # print the gridworld\n",
    "\n",
    "# here, returns is an average return value over the entire grid for each episode.\n",
    "# (ex. returns[0] is the avg. return from ep 1, returns[1] is the avg. return from ep 2)\n",
    "#(Q, returns, times) = approximate_q_mc(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)\n",
    "\n",
    "Q = approximate_q_sarsa(g, ArgMaxAgent(), num_episodes=5000, track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "|→|↓|→|↓|←|\n",
      "===========\n",
      "|↑|↑|↑|↓|↑|\n",
      "===========\n",
      "|↓|←|→|→|↓|\n",
      "===========\n",
      "|→|↑|←|↑|↓|\n",
      "===========\n",
      "|↑|↑|↑|→|↑|\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# plot maximum Q values\n",
    "(q_values, actions) = max_q(Q)\n",
    "plt.imshow(q_values)\n",
    "plt.colorbar()\n",
    "plt.title(\"Q Approximation\")\n",
    "plt.show()\n",
    "print_policy_from_q(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9301c5b6",
   "metadata": {},
   "source": [
    "### 3 Visualizing Variance-Bias Trade-Off\n",
    "Pick some average return, which constitures roughly the half-way-point between your algorithms average starting return and fully trained return. For both MC- control (from last weeks homework) and 1-step SARSA, do the following: (pick the same state for both!)\n",
    "* For both SARSA and MC-Control:\n",
    "    - Sample 1000 or more episodes starting at some specific (e.g. the starting) state, with some specific action\n",
    "    - Update only this specific starting Q-value!\n",
    "    - Track how the Q-value changes over the episodes (i.e. provide a list or ndarray with an estimation aver each episode)\n",
    "* Repeat the above 100 (or more) times for both SARSA and MC-Control 2\n",
    "* For both SARSA and MC-Control, create a lineplot including mean and std estimation (over the 100+ repeats) vs. episodes sampled\n",
    "* Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
