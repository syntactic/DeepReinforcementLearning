{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e0042a",
   "metadata": {},
   "source": [
    "# DRL Homework 03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4d76a4",
   "metadata": {},
   "source": [
    "### 1 Homework Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a91ad9",
   "metadata": {},
   "source": [
    "### 2 Learning a policy via 1-step SARSA\n",
    "For the following work again work with your own gridworld implementation! You may revise/change pieces of it, or ask other groups for access to their implementation of course.\n",
    "* Implement tabular 1-step SARSA control\n",
    "* Measure average Return-per-Episode and plot it against (1) episodes sampled, and (2) wallclock-time\n",
    "\n",
    "For an outstanding submission:\n",
    "\n",
    "* Visualize the State-Action Values in your gridworld during training at regular intervals, and provide a visualization of them (e.g. a series of images, best combine them into a short video clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c98ebc3-a5dc-46c8-87eb-82bb5add19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "DIRECTION_TO_SYMBOL = {UP:\"↑\", LEFT:\"←\", DOWN:\"↓\", RIGHT:\"→\", \"EMPTY\":\"█\"}\n",
    "\n",
    "def distance_between_states(state_1: State, state_2: State) -> int:\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2.row - state_1.row) + abs(state_2.col - state_1.col)\n",
    "    \n",
    "def direction_arithmetic(curr_pos: State, direction: int) -> State:\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos.row, curr_pos.col\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return State(row, col)\n",
    "\n",
    "def average(l):\n",
    "    if l:\n",
    "        return sum(l)/len(l)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def argmax(d):\n",
    "    try:\n",
    "        return max(d, key=d.get)\n",
    "    except ValueError:\n",
    "        return \"EMPTY\"\n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()\n",
    "        \n",
    "def print_policy_from_q(q):\n",
    "    s = \"\"\n",
    "    height = len(q)\n",
    "    width = len(q[0])\n",
    "    for row in range(height):\n",
    "        s += \"==\" * (width) + \"=\"\n",
    "        s += \"\\n\"\n",
    "        for col in range(width):\n",
    "            s += f\"|{DIRECTION_TO_SYMBOL[argmax(q[row][col])]}\"\n",
    "        s +=\"|\\n\"\n",
    "    s += \"==\" * (width) + \"=\"\n",
    "    print(s)\n",
    "\n",
    "def max_q(Q):\n",
    "    \"\"\" return a widthxheight array of the maximum returns for each gridcell\n",
    "        in Q and a widthxheight array of the best actions \"\"\"\n",
    "    width = len(Q)\n",
    "    height = len(Q[0])\n",
    "    \n",
    "    values = [[ float(\"-inf\") for _ in range(width)] for _ in range(height)] \n",
    "    actions = [[ [] for _ in range(width)] for _ in range(height)] \n",
    "    \n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            cell_returns = Q[x][y]\n",
    "            if cell_returns:\n",
    "                max_action = max(cell_returns, key= lambda x: cell_returns[x])\n",
    "                max_return = cell_returns[max_action]\n",
    "            \n",
    "                values[x][y] = max_return\n",
    "                actions[x][y] = max_action\n",
    "            \n",
    "    return (values, actions)\n",
    "\n",
    "class State:\n",
    "\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.col == other.col\n",
    "    \n",
    "    def __repr(self):\n",
    "        return f\"({self.row}, {self.col})\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7efce3ae",
   "metadata": {},
   "source": [
    "#### Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab2634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"This base agent class just takes random actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "    def get_action_from_policy(self):\n",
    "        return random.choice(self.available_actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        action_index = self.available_action.index(action)\n",
    "        self.state = self.available_next_states[action_index]\n",
    "        \n",
    "    def reset(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "class MagneticAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self):\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, self.win_state), self.available_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        return random.choices(self.available_actions, weights=reciprocals_of_distances)[0]\n",
    "    \n",
    "class ArgMaxAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self, q_state, epsilon):\n",
    "        chance = random.random()\n",
    "        # TIM: I think the conditions need to come in the opposite order if we want to explore\n",
    "        #if not q_state or chance > (1 - epsilon): # if the state has no information about action returns\n",
    "        if chance > (1 - epsilon) or not q_state:\n",
    "            # randomly pick an action\n",
    "            action = random.choice(self.available_actions)\n",
    "        else:\n",
    "            # get the action with the maximum return value\n",
    "            potential_action = argmax(q_state)\n",
    "            if potential_action in self.available_actions:\n",
    "                action = potential_action\n",
    "            else:\n",
    "                action = random.choice(self.available_actions)\n",
    "        return action\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e3595e7",
   "metadata": {},
   "source": [
    "#### GridWorld class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca3f081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False, win_state=None, start_state=None):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[\" \" for _ in range(width)] for _ in range(height)]\n",
    "        self.start_state = State(0,0) # just for initialization\n",
    "       \n",
    "        \"\"\" initialize the win_state as a random position in the grid, or if an argument \n",
    "            if the argument is provided, as the input win_state \"\"\"\n",
    "        \n",
    "        if win_state is None:\n",
    "            self.win_state = self.random_position()\n",
    "        else:\n",
    "            self.win_state = win_state\n",
    "            \n",
    "        self.grid[self.win_state.row][self.win_state.col] = \"W\"\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < 2:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < 2:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "        \n",
    "        \n",
    "    def set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return State(rand_row, rand_col)\n",
    "    \n",
    "    def state_is_open(self, state: State):\n",
    "        return self.grid[state.row][state.col] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        random_state = self.random_position()\n",
    "        if (self.state_is_open(random_state) and\n",
    "            distance_between_states(random_state, self.win_state) > 1 and \n",
    "            distance_between_states(random_state, self.start_state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"█\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset_agent(self, start=State(0,0)):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        if self.state_is_open(start):\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            self.start_state = start\n",
    "            self.agent.reset(start)\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "            \n",
    "        else:\n",
    "            sys.exit('reset_agent failed')\n",
    "           \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        return (state.row >=0 and state.row < self.height) and (state.col >=0 and state.col < self.width)\n",
    "    \n",
    "    def update_valid_next_actions_and_states(self):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(self.agent.state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        self.agent.available_actions = valid_actions\n",
    "        self.agent.available_next_states = valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent.state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent.state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent.state = self.start_state\n",
    "            else:\n",
    "                self.agent.state = target_state\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "    \n",
    "    \n",
    "    def random_start(self):\n",
    "        \"\"\" select a random state/action that is valid \"\"\"\n",
    "        state_valid = False\n",
    "        actions_possible = False\n",
    "    \n",
    "        while not state_valid and not actions_possible:\n",
    "        \n",
    "            \"\"\" select a random position on the grid as a potential starting state and check that it is open\"\"\"\n",
    "            random_state = self.random_position()\n",
    "            state_valid = self.state_is_open(random_state)\n",
    "            \n",
    "            \"\"\" If the state is open, move the agent to the state and check if there are availble moves \"\"\"\n",
    "            if(state_valid):\n",
    "                self.reset_agent(random_state)\n",
    "                action_possible = len(self.agent.available_actions) > 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a76ba88e",
   "metadata": {},
   "source": [
    "#### Approximating Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f727db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q(gridworld, agent, gamma=0.95, num_episodes=500, track=False):\n",
    "    \"\"\"Approximates Q, a gridworld.width by gridworld.height by actions matrix that \n",
    "       contains the average returns from taking each action in each of the possible\n",
    "       grid states across num_episodes \"\"\"\n",
    "    \n",
    "    \"\"\"Initialize Q and returns as matrices that are width * height * actions to store returns\n",
    "       calculated during each episode. For each state/action pair in the returns matrix, a list \n",
    "       of returns for each episode will accumulate. For each state/action pair in the Q matrix,\n",
    "       the average return across episodes will be stored. A list of wallclock_times are stored\n",
    "       to keep track of the time elapsed per episode\"\"\"\n",
    "    Q = [[ {} for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    returns = [[ { } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    episode_returns = []\n",
    "    avg_returns_per_episode = []\n",
    "    wallclock_times = []\n",
    "    \n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        \"\"\"Initialize the time step (t) for the current episode and create lists to store\n",
    "            visited states and taken actions (index matched) \"\"\"\n",
    "        time_step = 0\n",
    "        visited_states_and_taken_actions = list()\n",
    "        \n",
    "        \"\"\"Get a random valid state to place the agent in and select a first action \"\"\"\n",
    "        gridworld.random_start()\n",
    "        selected_action = random.choice(gridworld.agent.available_actions) \n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            \n",
    "            reward_from_action = gridworld.reward(selected_action)\n",
    "\n",
    "            for i in range(len(visited_states_and_taken_actions)):\n",
    "                state_in_history = visited_states_and_taken_actions[-1*i] # moving backwards in time\n",
    "                state_in_history[1] += gamma**i * reward_from_action # element 1 is the cumulative reward\n",
    "                \n",
    "            \"\"\"Store the current state/action pair \"\"\"\n",
    "            visited_states_and_taken_actions.append([(gridworld.agent.state, selected_action), reward_from_action])\n",
    "            \n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            gridworld.move(selected_action)\n",
    "            time_step += 1\n",
    "            \n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            selected_action = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                     [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "        \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "           by (row, col)). Then recalculate Q based on the ever growing returns lists. As they grow, the\n",
    "           values in Q should converge.\"\"\"\n",
    "        \n",
    "        \"\"\"The return of the episode == The return of the initial (s, a) pair that we chose.\n",
    "        I considered summing all returns from the episode but I don't think that's right.\"\"\"\n",
    "        episode_returns.append(visited_states_and_taken_actions[0][1])\n",
    "        avg_returns_per_episode.append(average(episode_returns))\n",
    "        \n",
    "        for i in range(1, len(visited_states_and_taken_actions)):\n",
    "            step_T_minus_i = visited_states_and_taken_actions[-1*i]\n",
    "            visited_state, taken_action = step_T_minus_i[0]\n",
    "            if (visited_state, taken_action) in map(lambda l : l[0], visited_states_and_taken_actions[0:-1*i]):\n",
    "                continue\n",
    "\n",
    "            rewards = step_T_minus_i[1]\n",
    "            if taken_action not in returns[visited_state.row][visited_state.col]:\n",
    "                returns[visited_state.row][visited_state.col][taken_action] = []\n",
    "            returns[visited_state.row][visited_state.col][taken_action].append(rewards)\n",
    "            Q[visited_state.row][visited_state.col][taken_action] = average(returns[visited_state.row][visited_state.col][taken_action])\n",
    "        \n",
    "        \n",
    "        #avg_returns_per_episode.append(average(returns_per_visited_state))\n",
    "        wallclock_times.append(time.time() - start_time)\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "        \n",
    "    return (Q, avg_returns_per_episode, wallclock_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70a25919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | |█| |█|\n",
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | | |W| |\n",
      "===========\n",
      "|*| |*| | |\n",
      "===========\n",
      "Completed episodes: 100\n",
      "Completed episodes: 200\n",
      "Completed episodes: 300\n",
      "Completed episodes: 400\n",
      "Completed episodes: 500\n",
      "Completed episodes: 600\n",
      "Completed episodes: 700\n",
      "Completed episodes: 800\n",
      "Completed episodes: 900\n",
      "Completed episodes: 1000\n",
      "Completed episodes: 1100\n",
      "Completed episodes: 1200\n",
      "Completed episodes: 1300\n",
      "Completed episodes: 1400\n",
      "Completed episodes: 1500\n",
      "Completed episodes: 1600\n",
      "Completed episodes: 1700\n",
      "Completed episodes: 1800\n",
      "Completed episodes: 1900\n",
      "Completed episodes: 2000\n",
      "Completed episodes: 2100\n",
      "Completed episodes: 2200\n",
      "Completed episodes: 2300\n",
      "Completed episodes: 2400\n",
      "Completed episodes: 2500\n",
      "Completed episodes: 2600\n",
      "Completed episodes: 2700\n",
      "Completed episodes: 2800\n",
      "Completed episodes: 2900\n",
      "Completed episodes: 3000\n",
      "Completed episodes: 3100\n",
      "Completed episodes: 3200\n",
      "Completed episodes: 3300\n",
      "Completed episodes: 3400\n",
      "Completed episodes: 3500\n",
      "Completed episodes: 3600\n",
      "Completed episodes: 3700\n",
      "Completed episodes: 3800\n",
      "Completed episodes: 3900\n",
      "Completed episodes: 4000\n",
      "Completed episodes: 4100\n",
      "Completed episodes: 4200\n",
      "Completed episodes: 4300\n",
      "Completed episodes: 4400\n",
      "Completed episodes: 4500\n",
      "Completed episodes: 4600\n",
      "Completed episodes: 4700\n",
      "Completed episodes: 4800\n",
      "Completed episodes: 4900\n",
      "Completed episodes: 5000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test approximate_q\"\"\"\n",
    "random.seed(0)\n",
    "height = 5\n",
    "width = 5\n",
    "num_episodes = 5000\n",
    "g = GridWorld(height, width, complex=True)\n",
    "print(g) # print the gridworld\n",
    "\n",
    "# here, returns is an average return value over the entire grid for each episode.\n",
    "# (ex. returns[0] is the avg. return from ep 1, returns[1] is the avg. return from ep 2)\n",
    "(Q, returns, times) = approximate_q(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ac0fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "|→|↓|→|↓|←|\n",
      "===========\n",
      "|→|↓|█|↓|█|\n",
      "===========\n",
      "|→|→|→|↓|↓|\n",
      "===========\n",
      "|↑|→|→|█|←|\n",
      "===========\n",
      "|█|↑|█|↑|↑|\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# plot maximum Q values\n",
    "(q_values, actions) = max_q(Q)\n",
    "plt.imshow(q_values)\n",
    "plt.colorbar()\n",
    "plt.title(\"Q Approximation\")\n",
    "plt.show()\n",
    "print_policy_from_q(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9301c5b6",
   "metadata": {},
   "source": [
    "### 3 Visualizing Variance-Bias Trade-Off\n",
    "Pick some average return, which constitures roughly the half-way-point between your algorithms average starting return and fully trained return. For both MC- control (from last weeks homework) and 1-step SARSA, do the following: (pick the same state for both!)\n",
    "* For both SARSA and MC-Control:\n",
    "    - Sample 1000 or more episodes starting at some specific (e.g. the starting) state, with some specific action\n",
    "    - Update only this specific starting Q-value!\n",
    "    - Track how the Q-value changes over the episodes (i.e. provide a list or ndarray with an estimation aver each episode)\n",
    "* Repeat the above 100 (or more) times for both SARSA and MC-Control 2\n",
    "* For both SARSA and MC-Control, create a lineplot including mean and std estimation (over the 100+ repeats) vs. episodes sampled\n",
    "* Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
