{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e0042a",
   "metadata": {},
   "source": [
    "# DRL Homework 03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4d76a4",
   "metadata": {},
   "source": [
    "### 1 Homework Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a91ad9",
   "metadata": {},
   "source": [
    "### 2 Learning a policy via 1-step SARSA\n",
    "For the following work again work with your own gridworld implementation! You may revise/change pieces of it, or ask other groups for access to their implementation of course.\n",
    "* Implement tabular 1-step SARSA control\n",
    "* Measure average Return-per-Episode and plot it against (1) episodes sampled, and (2) wallclock-time\n",
    "\n",
    "For an outstanding submission:\n",
    "\n",
    "* Visualize the State-Action Values in your gridworld during training at regular intervals, and provide a visualization of them (e.g. a series of images, best combine them into a short video clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c98ebc3-a5dc-46c8-87eb-82bb5add19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "DIRECTION_TO_SYMBOL = {UP:\"↑\", LEFT:\"←\", DOWN:\"↓\", RIGHT:\"→\", \"EMPTY\":\"█\"}\n",
    "\n",
    "def distance_between_states(state_1: State, state_2: State) -> int:\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2.row - state_1.row) + abs(state_2.col - state_1.col)\n",
    "    \n",
    "def direction_arithmetic(curr_pos: State, direction: int) -> State:\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos.row, curr_pos.col\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return State(row, col)\n",
    "\n",
    "def average(l):\n",
    "    if l:\n",
    "        return sum(l)/len(l)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def argmax(d):\n",
    "    try:\n",
    "        return max(d, key=d.get)\n",
    "    except ValueError:\n",
    "        return \"EMPTY\"\n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()\n",
    "        \n",
    "def print_policy_from_q(q):\n",
    "    s = \"\"\n",
    "    height = len(q)\n",
    "    width = len(q[0])\n",
    "    for row in range(height):\n",
    "        s += \"==\" * (width) + \"=\"\n",
    "        s += \"\\n\"\n",
    "        for col in range(width):\n",
    "            s += f\"|{DIRECTION_TO_SYMBOL[argmax(q[row][col])]}\"\n",
    "        s +=\"|\\n\"\n",
    "    s += \"==\" * (width) + \"=\"\n",
    "    print(s)\n",
    "\n",
    "# can we make the below function a method to gridworld instead?\n",
    "def random_start(gridworld):\n",
    "    ''' select a random state/action that is valid '''\n",
    "    state_valid = False\n",
    "    actions_possible = False\n",
    "    \n",
    "    while not state_valid and not actions_possible:\n",
    "        \n",
    "        \"\"\" select a random position on the grid as a potential starting state and check that it is open\"\"\"\n",
    "        state = gridworld.random_position()\n",
    "        state_valid = gridworld.state_is_open(state)\n",
    "        \n",
    "        \"\"\" If the state is open, move the agent to the state and check if there are availble moves \"\"\"\n",
    "        if(state_valid):\n",
    "            gridworld.reset_agent(state)\n",
    "            action_possible = len(gridworld.agent.available_actions) > 0\n",
    "\n",
    "def max_q(Q):\n",
    "    \"\"\" return a widthxheight array of the maximum returns for each gridcell\n",
    "        in Q and a widthxheight array of the best actions \"\"\"\n",
    "    width = len(Q)\n",
    "    height = len(Q[0])\n",
    "    \n",
    "    values = [[ float(\"-inf\") for _ in range(width)] for _ in range(height)] \n",
    "    actions = [[ [] for _ in range(width)] for _ in range(height)] \n",
    "    \n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            cell_returns = Q[x][y]\n",
    "            if cell_returns:\n",
    "                max_action = max(cell_returns, key= lambda x: cell_returns[x])\n",
    "                max_return = cell_returns[max_action]\n",
    "            \n",
    "                values[x][y] = max_return\n",
    "                actions[x][y] = max_action\n",
    "            \n",
    "    return (values, actions)\n",
    "\n",
    "class State:\n",
    "\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.col == other.col\n",
    "    \n",
    "    def __repr(self):\n",
    "        return f\"({self.row}, {self.col})\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7efce3ae",
   "metadata": {},
   "source": [
    "#### Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab2634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"This base agent class just takes random actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "    def get_action_from_policy(self):\n",
    "        return random.choice(self.available_actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        action_index = self.available_action.index(action)\n",
    "        self.state = self.available_next_states[action_index]\n",
    "        \n",
    "    def reset(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "class MagneticAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self):\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, self.win_state), self.available_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        return random.choices(self.available_actions, weights=reciprocals_of_distances)[0]\n",
    "    \n",
    "class ArgMaxAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self, q_state, epsilon):\n",
    "        chance = random.random()\n",
    "        # TIM: I think the conditions need to come in the opposite order if we want to explore\n",
    "        #if not q_state or chance > (1 - epsilon): # if the state has no information about action returns\n",
    "        if chance > (1 - epsilon) or not q_state:\n",
    "            # randomly pick an action\n",
    "            action = random.choice(self.available_actions)\n",
    "        else:\n",
    "            # get the action with the maximum return value\n",
    "            potential_action = argmax(q_state)\n",
    "            if potential_action in self.available_actions:\n",
    "                action = potential_action\n",
    "            else:\n",
    "                action = random.choice(self.available_actions)\n",
    "        return action\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e3595e7",
   "metadata": {},
   "source": [
    "#### GridWorld class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca3f081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False, win_state=None, start_state=None):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[\" \" for _ in range(width)] for _ in range(height)]\n",
    "        self.start_state = State(0,0) # just for initialization\n",
    "       \n",
    "        \"\"\" initialize the win_state as a random position in the grid, or if an argument \n",
    "            if the argument is provided, as the input win_state \"\"\"\n",
    "        \n",
    "        if win_state is None:\n",
    "            self.win_state = self.random_position()\n",
    "        else:\n",
    "            self.win_state = win_state\n",
    "            \n",
    "        self.grid[self.win_state.row][self.win_state.col] = \"W\"\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < 2:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < 2:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "        \n",
    "        \n",
    "    def set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return State(rand_row, rand_col)\n",
    "    \n",
    "    def state_is_open(self, state: State):\n",
    "        return self.grid[state.row][state.col] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        random_state = self.random_position()\n",
    "        if (self.state_is_open(random_state) and\n",
    "            distance_between_states(random_state, self.win_state) > 1 and \n",
    "            distance_between_states(random_state, self.start_state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"█\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset_agent(self, start=State(0,0)):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        if self.state_is_open(start):\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            self.start_state = start\n",
    "            self.agent.reset(start)\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "            \n",
    "        else:\n",
    "            sys.exit('reset_agent failed')\n",
    "           \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        return (state.row >=0 and state.row < self.height) and (state.col >=0 and state.col < self.width)\n",
    "    \n",
    "    def update_valid_next_actions_and_states(self):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(self.agent.state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        self.agent.available_actions = valid_actions\n",
    "        self.agent.available_next_states = valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent.state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent.state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent.state = self.start_state\n",
    "            else:\n",
    "                self.agent.state = target_state\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s\n",
    "    \n",
    "    def random_start(self):\n",
    "        \"\"\" select a random state/action that is valid \"\"\"\n",
    "        state_valid = False\n",
    "        actions_possible = False\n",
    "    \n",
    "        while not state_valid and not actions_possible:\n",
    "        \n",
    "            \"\"\" select a random position on the grid as a potential starting state and check that it is open\"\"\"\n",
    "            random_state = self.random_position()\n",
    "            state_valid = self.state_is_open(random_state)\n",
    "            \n",
    "            \"\"\" If the state is open, move the agent to the state and check if there are availble moves \"\"\"\n",
    "            if(state_valid):\n",
    "                self.reset_agent(state)\n",
    "                action_possible = len(self.agent.available_actions) > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a76ba88e",
   "metadata": {},
   "source": [
    "#### Approximating Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f727db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q(gridworld, agent, gamma=0.95, num_episodes=500, track=False):\n",
    "    \"\"\"Approximates Q, a gridworld.width by gridworld.height by actions matrix that \n",
    "       contains the average returns from taking each action in each of the possible\n",
    "       grid states across num_episodes \"\"\"\n",
    "    \n",
    "    \"\"\"Initialize Q and returns as matrices that are width * height * actions to store returns\n",
    "       calculated during each episode. For each state/action pair in the returns matrix, a list \n",
    "       of returns for each episode will accumulate. For each state/action pair in the Q matrix,\n",
    "       the average return across episodes will be stored. A list of wallclock_times are stored\n",
    "       to keep track of the time elapsed per episode\"\"\"\n",
    "    Q = [[ {} for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    returns = [[ { } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    episode_returns = []\n",
    "    avg_returns_per_episode = []\n",
    "    wallclock_times = []\n",
    "    \n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        \"\"\"Initialize the time step (t) for the current episode and create lists to store\n",
    "            visited states and taken actions (index matched) \"\"\"\n",
    "        time_step = 0\n",
    "        visited_states_and_taken_actions = list()\n",
    "        \n",
    "        \"\"\"Get a random valid state to place the agent in and select a first action \"\"\"\n",
    "        random_start(gridworld)\n",
    "        selected_action = random.choice(gridworld.agent.available_actions) \n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            \n",
    "            reward_from_action = gridworld.reward(selected_action)\n",
    "\n",
    "            for i in range(len(visited_states_and_taken_actions)):\n",
    "                state_in_history = visited_states_and_taken_actions[-1*i] # moving backwards in time\n",
    "                state_in_history[1] += gamma**i * reward_from_action # element 1 is the cumulative reward\n",
    "                \n",
    "            \"\"\"Store the current state/action pair \"\"\"\n",
    "            visited_states_and_taken_actions.append([(gridworld.agent.state, selected_action), reward_from_action])\n",
    "            \n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            gridworld.move(selected_action)\n",
    "            time_step += 1\n",
    "            \n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            selected_action = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                     [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "        \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "           by (row, col)). Then recalculate Q based on the ever growing returns lists. As they grow, the\n",
    "           values in Q should converge.\"\"\"\n",
    "        \n",
    "        \"\"\"The return of the episode == The return of the initial (s, a) pair that we chose.\n",
    "        I considered summing all returns from the episode but I don't think that's right.\"\"\"\n",
    "        episode_returns.append(visited_states_and_taken_actions[0][1])\n",
    "        avg_returns_per_episode.append(average(episode_returns))\n",
    "        \n",
    "        for i in range(1, len(visited_states_and_taken_actions)):\n",
    "            step_T_minus_i = visited_states_and_taken_actions[-1*i]\n",
    "            visited_state, taken_action = step_T_minus_i[0]\n",
    "            if (visited_state, taken_action) in map(lambda l : l[0], visited_states_and_taken_actions[0:-1*i]):\n",
    "                continue\n",
    "\n",
    "            rewards = step_T_minus_i[1]\n",
    "            if taken_action not in returns[visited_state.row][visited_state.col]:\n",
    "                returns[visited_state.row][visited_state.col][taken_action] = []\n",
    "            returns[visited_state.row][visited_state.col][taken_action].append(rewards)\n",
    "            Q[visited_state.row][visited_state.col][taken_action] = average(returns[visited_state.row][visited_state.col][taken_action])\n",
    "        \n",
    "        \n",
    "        #avg_returns_per_episode.append(average(returns_per_visited_state))\n",
    "        wallclock_times.append(time.time() - start_time)\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "        \n",
    "    return (Q, avg_returns_per_episode, wallclock_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70a25919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | |█| |█|\n",
      "===========\n",
      "| | | | | |\n",
      "===========\n",
      "| | | |W| |\n",
      "===========\n",
      "|*| |*| | |\n",
      "===========\n",
      "Completed episodes: 100\n",
      "Completed episodes: 200\n",
      "Completed episodes: 300\n",
      "Completed episodes: 400\n",
      "Completed episodes: 500\n",
      "Completed episodes: 600\n",
      "Completed episodes: 700\n",
      "Completed episodes: 800\n",
      "Completed episodes: 900\n",
      "Completed episodes: 1000\n",
      "Completed episodes: 1100\n",
      "Completed episodes: 1200\n",
      "Completed episodes: 1300\n",
      "Completed episodes: 1400\n",
      "Completed episodes: 1500\n",
      "Completed episodes: 1600\n",
      "Completed episodes: 1700\n",
      "Completed episodes: 1800\n",
      "Completed episodes: 1900\n",
      "Completed episodes: 2000\n",
      "Completed episodes: 2100\n",
      "Completed episodes: 2200\n",
      "Completed episodes: 2300\n",
      "Completed episodes: 2400\n",
      "Completed episodes: 2500\n",
      "Completed episodes: 2600\n",
      "Completed episodes: 2700\n",
      "Completed episodes: 2800\n",
      "Completed episodes: 2900\n",
      "Completed episodes: 3000\n",
      "Completed episodes: 3100\n",
      "Completed episodes: 3200\n",
      "Completed episodes: 3300\n",
      "Completed episodes: 3400\n",
      "Completed episodes: 3500\n",
      "Completed episodes: 3600\n",
      "Completed episodes: 3700\n",
      "Completed episodes: 3800\n",
      "Completed episodes: 3900\n",
      "Completed episodes: 4000\n",
      "Completed episodes: 4100\n",
      "Completed episodes: 4200\n",
      "Completed episodes: 4300\n",
      "Completed episodes: 4400\n",
      "Completed episodes: 4500\n",
      "Completed episodes: 4600\n",
      "Completed episodes: 4700\n",
      "Completed episodes: 4800\n",
      "Completed episodes: 4900\n",
      "Completed episodes: 5000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test approximate_q\"\"\"\n",
    "random.seed(0)\n",
    "height = 5\n",
    "width = 5\n",
    "num_episodes = 5000\n",
    "g = GridWorld(height, width, complex=True)\n",
    "print(g) # print the gridworld\n",
    "\n",
    "# here, returns is an average return value over the entire grid for each episode.\n",
    "# (ex. returns[0] is the avg. return from ep 1, returns[1] is the avg. return from ep 2)\n",
    "(Q, returns, times) = approximate_q(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ac0fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "|↓|↓|→|↓|←|\n",
      "===========\n",
      "|→|↓|█|↓|█|\n",
      "===========\n",
      "|→|→|→|↓|↓|\n",
      "===========\n",
      "|→|→|→|█|←|\n",
      "===========\n",
      "|█|↑|█|↑|↑|\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# plot maximum Q values\n",
    "(q_values, actions) = max_q(Q)\n",
    "plt.imshow(q_values)\n",
    "plt.colorbar()\n",
    "plt.title(\"Q Approximation\")\n",
    "plt.show()\n",
    "print_policy_from_q(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9301c5b6",
   "metadata": {},
   "source": [
    "### 3 Visualizing Variance-Bias Trade-Off\n",
    "Pick some average return, which constitures roughly the half-way-point between your algorithms average starting return and fully trained return. For both MC- control (from last weeks homework) and 1-step SARSA, do the following: (pick the same state for both!)\n",
    "* For both SARSA and MC-Control:\n",
    "    - Sample 1000 or more episodes starting at some specific (e.g. the starting) state, with some specific action\n",
    "    - Update only this specific starting Q-value!\n",
    "    - Track how the Q-value changes over the episodes (i.e. provide a list or ndarray with an estimation aver each episode)\n",
    "* Repeat the above 100 (or more) times for both SARSA and MC-Control 2\n",
    "* For both SARSA and MC-Control, create a lineplot including mean and std estimation (over the 100+ repeats) vs. episodes sampled\n",
    "* Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
