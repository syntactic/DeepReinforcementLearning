{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e0042a",
   "metadata": {},
   "source": [
    "# DRL Homework 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d76a4",
   "metadata": {},
   "source": [
    "### 1 Homework Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a91ad9",
   "metadata": {},
   "source": [
    "### 2 Learning a policy via 1-step SARSA\n",
    "For the following work again work with your own gridworld implementation! You may revise/change pieces of it, or ask other groups for access to their implementation of course.\n",
    "* Implement tabular 1-step SARSA control\n",
    "* Measure average Return-per-Episode and plot it against (1) episodes sampled, and (2) wallclock-time\n",
    "\n",
    "For an outstanding submission:\n",
    "\n",
    "* Visualize the State-Action Values in your gridworld during training at regular intervals, and provide a visualization of them (e.g. a series of images, best combine them into a short video clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1c98ebc3-a5dc-46c8-87eb-82bb5add19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Directions as a faux enum, this is the set A of actions\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "DOWN = 2\n",
    "RIGHT = 3\n",
    "DIRECTIONS = [UP, LEFT, DOWN, RIGHT]\n",
    "DIRECTION_TO_SYMBOL = {UP:\"↑\", LEFT:\"←\", DOWN:\"↓\", RIGHT:\"→\", \"EMPTY\":\"█\"}\n",
    "\n",
    "def distance_between_states(state_1: State, state_2: State) -> int:\n",
    "    \"\"\"Calculate Manhattan distance between states. This is useful for the \n",
    "       agent's policy, but also useful for positioning walls are warps away \n",
    "       from the start and end state.\"\"\"\n",
    "    return abs(state_2.row - state_1.row) + abs(state_2.col - state_1.col)\n",
    "    \n",
    "def direction_arithmetic(curr_pos: State, direction: int) -> State:\n",
    "    \"\"\"Calculate the resulting state coordinates given a state and direction.\"\"\"\n",
    "    row, col = curr_pos.row, curr_pos.col\n",
    "    if direction == UP:\n",
    "        row = row - 1\n",
    "    elif direction == LEFT:\n",
    "        col = col - 1\n",
    "    elif direction == DOWN:\n",
    "        row = row + 1\n",
    "    elif direction == RIGHT:\n",
    "        col = col + 1\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized direction: {direction}\")\n",
    "    return State(row, col)\n",
    "\n",
    "def average(l):\n",
    "    if l:\n",
    "        return sum(l)/len(l)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def argmax(d):\n",
    "    \n",
    "    \"\"\" get index matched lists of actions and values for d \"\"\"\n",
    "    actions = list(d.keys())\n",
    "    values = list(d.values())\n",
    "    \n",
    "    try:\n",
    "        max_val = max(values)\n",
    "    except ValueError:\n",
    "        return \"EMPTY\"\n",
    "    \n",
    "    max_actions = []\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        if values[i] == max_val:\n",
    "            max_actions.append(actions[i])\n",
    "            \n",
    "    return random.choice(max_actions)\n",
    "    \n",
    "    \n",
    "\n",
    "def print_matrix(m):\n",
    "    for row in m:\n",
    "        for col in row:\n",
    "            print(f\"{col:.3f}\", end='\\t')\n",
    "        print()\n",
    "        \n",
    "def print_policy_from_q(q):\n",
    "    \"\"\" prints the policy from q, excluding coordinates in blank coords \"\"\"\n",
    "    s = \"\"\n",
    "    height = len(q)\n",
    "    width = len(q[0])\n",
    "    for row in range(height):\n",
    "        s += \"==\" * (width) + \"=\"\n",
    "        s += \"\\n\"\n",
    "        for col in range(width):\n",
    "            s += f\"|{DIRECTION_TO_SYMBOL[argmax(q[row][col])]}\"\n",
    "        s +=\"|\\n\"\n",
    "    s += \"==\" * (width) + \"=\"\n",
    "    print(s)\n",
    "    \n",
    "def grid_to_ascii(grid, transformation=str):\n",
    "    s = \"\"\n",
    "    height = len(grid)\n",
    "    width = len(grid[0])\n",
    "    for row in range(height):\n",
    "        s += \"==\" * (width) + \"=\"\n",
    "        s += \"\\n\"\n",
    "        for col in range(width):\n",
    "            s += f\"|{transformation(grid[row][col])]}\"\n",
    "        s +=\"|\\n\"\n",
    "    s += \"==\" * (width) + \"=\"\n",
    "    return s\n",
    "    \n",
    "\n",
    "def max_q(Q):\n",
    "    \"\"\" return a widthxheight array of the maximum returns for each gridcell\n",
    "        in Q and a widthxheight array of the best actions \"\"\"\n",
    "    width = len(Q)\n",
    "    height = len(Q[0])\n",
    "    \n",
    "    values = [[ float(\"-inf\") for _ in range(width)] for _ in range(height)] \n",
    "    actions = [[ [] for _ in range(width)] for _ in range(height)] \n",
    "    \n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            cell_returns = Q[x][y]\n",
    "            if cell_returns:\n",
    "                max_action = max(cell_returns, key= lambda x: cell_returns[x])\n",
    "                max_return = cell_returns[max_action]\n",
    "            \n",
    "                values[x][y] = max_return\n",
    "                actions[x][y] = max_action\n",
    "            \n",
    "    return (values, actions)\n",
    "               \n",
    "class State:\n",
    "\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.col == other.col\n",
    "    \n",
    "    def __repr(self):\n",
    "        return f\"({self.row}, {self.col})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efce3ae",
   "metadata": {},
   "source": [
    "#### Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ab2634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"This base agent class just takes random actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "    def get_action_from_policy(self):\n",
    "        return random.choice(self.available_actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        action_index = self.available_action.index(action)\n",
    "        self.state = self.available_next_states[action_index]\n",
    "        \n",
    "    def reset(self, state=State(0, 0)):\n",
    "        self.state = state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "        \n",
    "class MagneticAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self):\n",
    "        \n",
    "        \"\"\"Get possible actions/next states, and pick one. The probability of choosing a direction is\n",
    "           inversely proportional to the distance that the resulting state is from the terminal state\"\"\"\n",
    "        distances_to_win_states = list(map(lambda s : distance_between_states(s, self.win_state), self.available_next_states))\n",
    "        reciprocals_of_distances = list(map(lambda d : 1/(d+1), distances_to_win_states))\n",
    "        sum_of_reciprocals = sum(reciprocals_of_distances)\n",
    "        normalized_probabilities = list(map(lambda r : r/sum_of_reciprocals, reciprocals_of_distances))\n",
    "        return random.choices(self.available_actions, weights=reciprocals_of_distances)[0]\n",
    "    \n",
    "class ArgMaxAgent(Agent):\n",
    "    \"\"\"The MagneticAgent likes to choose actions that bring it closer\n",
    "       to the win state, regardless of what obstacles are in the way.\"\"\"\n",
    "\n",
    "    def __init__(self, win_state=State(0, 0), start_state=State(0, 0)):\n",
    "        self.state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.available_actions = []\n",
    "        self.available_next_states = []\n",
    "    \n",
    "    def get_action_from_policy(self, q_state, epsilon):\n",
    "        chance = random.random()\n",
    "        # TIM: I think the conditions need to come in the opposite order if we want to explore\n",
    "        #if not q_state or chance > (1 - epsilon): # if the state has no information about action returns\n",
    "        if chance > (1 - epsilon) or not q_state:\n",
    "            # randomly pick an action\n",
    "            action = random.choice(self.available_actions)\n",
    "        else:\n",
    "            # get the action with the maximum return value\n",
    "            potential_action = argmax(q_state)\n",
    "            if potential_action in self.available_actions:\n",
    "                action = potential_action\n",
    "            else:\n",
    "                action = random.choice(self.available_actions)\n",
    "        return action\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3595e7",
   "metadata": {},
   "source": [
    "#### GridWorld class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ca3f081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    " \n",
    "    def __init__(self, height, width, complex=False, num_walls=2, num_warps=2, win_state=None, start_state=None):\n",
    "        \"\"\"Initialize the grid with properties we expect to not change\n",
    "           during the game.\"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.complex = complex\n",
    "        self.num_walls = num_walls\n",
    "        self.num_warps = num_warps\n",
    "        self.walls = []\n",
    "        self.warps = []\n",
    "        self.grid = [[\" \" for _ in range(width)] for _ in range(height)]\n",
    "        self.start_state = State(0,0) # just for initialization\n",
    "       \n",
    "        \"\"\" initialize the win_state as a random position in the grid, or if an argument \n",
    "            if the argument is provided, as the input win_state \"\"\"\n",
    "        \n",
    "        if win_state is None:\n",
    "            self.win_state = self.random_position()\n",
    "        else:\n",
    "            self.win_state = win_state\n",
    "            \n",
    "        self.grid[self.win_state.row][self.win_state.col] = \"W\"\n",
    "        \n",
    "        \"\"\"Add complexities (2 walls, 2 warps). the location is random, but consistent for\n",
    "           a given grid size. This helps make the value function more specific to one grid.\"\"\"\n",
    "        if self.complex:\n",
    "            iteration = 0\n",
    "            while len(self.walls) < self.num_walls:\n",
    "                self.spawn_complexity_randomly(\"wall\", iteration)\n",
    "                iteration += 1\n",
    "            while len(self.warps) < self.num_warps:\n",
    "                self.spawn_complexity_randomly(\"warp\", iteration)\n",
    "                iteration += 1\n",
    "            random.seed()\n",
    "        \n",
    "        \n",
    "    def set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def random_position(self):\n",
    "        \"\"\"Pick out a random tile.\"\"\"\n",
    "        rand_row = random.randint(0, self.height-1)\n",
    "        rand_col = random.randint(0, self.width-1)\n",
    "        return State(rand_row, rand_col)\n",
    "    \n",
    "    def state_is_open(self, state: State):\n",
    "        return self.grid[state.row][state.col] == \" \"\n",
    "    \n",
    "    def spawn_complexity_randomly(self, complexity, seed=None):\n",
    "        random.seed(seed)\n",
    "        random_state = self.random_position()\n",
    "        if (self.state_is_open(random_state) and\n",
    "            distance_between_states(random_state, self.win_state) > 1 and \n",
    "            distance_between_states(random_state, self.start_state) > 1):\n",
    "            if complexity == \"wall\":\n",
    "                self.walls.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"█\"\n",
    "            elif complexity == \"warp\":\n",
    "                self.warps.append(random_state)\n",
    "                self.grid[random_state.row][random_state.col] = \"*\"\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized complexity: {complexity}!\")\n",
    "        \n",
    "    def reset_agent(self, start=State(0,0)):\n",
    "        \"\"\"Reset the GridWorld. Send the agent back to the corner. Set up\n",
    "           walls and warps\"\"\"\n",
    "        if self.state_is_open(start):\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            self.start_state = start\n",
    "            self.agent.reset(start)\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "            \n",
    "        else:\n",
    "            sys.exit('reset_agent failed')\n",
    "           \n",
    "    def valid(self, state):\n",
    "        \"\"\"Checks to see if a state lies within the bounds of the grid.\"\"\"\n",
    "        return (state.row >=0 and state.row < self.height) and (state.col >=0 and state.col < self.width)\n",
    "    \n",
    "    def update_valid_next_actions_and_states(self):\n",
    "        \"\"\"From the agent's state or a given state, look around and see what directions\n",
    "           are possible.\"\"\"\n",
    "        valid_actions = []\n",
    "        valid_states = []\n",
    "        for direction in DIRECTIONS:\n",
    "            target_state = direction_arithmetic(self.agent.state, direction)\n",
    "            if self.valid(target_state):\n",
    "                valid_actions.append(direction)\n",
    "                valid_states.append(target_state)\n",
    "        self.agent.available_actions = valid_actions\n",
    "        self.agent.available_next_states = valid_states\n",
    "        \n",
    "    def reward_from_state(self, state, direction):\n",
    "        \"\"\"Reward function given state and action. Penalizes warps more than walls.\n",
    "           No penalty for simply moving to an open space.\"\"\"\n",
    "        target_state = direction_arithmetic(state, direction)\n",
    "        if target_state == self.win_state:\n",
    "            return 1\n",
    "        if target_state in self.walls:\n",
    "            return -0.25\n",
    "        if target_state in self.warps:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def reward(self, direction):\n",
    "        \"\"\"Same as above, but from the agent's state.\"\"\"\n",
    "        return self.reward_from_state(self.agent.state, direction)\n",
    "    \n",
    "        \n",
    "    def move(self, direction):\n",
    "        \"\"\"Try to move in a given direction. Hitting a wall will leave the agent where\n",
    "           it is. Hitting a warp will send the agent back to the starting corner.\"\"\"\n",
    "        target_state = direction_arithmetic(self.agent.state, direction)\n",
    "        if self.valid(target_state) and target_state not in self.walls:\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \" \"\n",
    "            \n",
    "            # go back to the beginning if you hit a warp tile\n",
    "            if target_state in self.warps:\n",
    "                self.agent.state = self.start_state\n",
    "            else:\n",
    "                self.agent.state = target_state\n",
    "            self.grid[self.agent.state.row][self.agent.state.col] = \"A\"\n",
    "            self.update_valid_next_actions_and_states()\n",
    "    \n",
    "    \n",
    "    def random_start(self):\n",
    "        \"\"\" select a random state/action that is valid \"\"\"\n",
    "        state_valid = False\n",
    "        actions_possible = False\n",
    "    \n",
    "        while not state_valid and not actions_possible:\n",
    "        \n",
    "            \"\"\" select a random position on the grid as a potential starting state and check that it is open\"\"\"\n",
    "            random_state = self.random_position()\n",
    "            state_valid = self.state_is_open(random_state)\n",
    "            \n",
    "            \"\"\" If the state is open, move the agent to the state and check if there are availble moves \"\"\"\n",
    "            if(state_valid):\n",
    "                self.reset_agent(random_state)\n",
    "                action_possible = len(self.agent.available_actions) > 0\n",
    "                \n",
    "    def init_q(self):\n",
    "        \"\"\" intializes a Q matrix with an arbitrary value for each possible action \"\"\"\n",
    "        arb_val = -0.3\n",
    "\n",
    "        Q = [[ {} for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        \"\"\" loops through each state in the grid\"\"\"\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                s = State(r, c)\n",
    "\n",
    "                if s in self.walls or s in self.warps or s == self.win_state:\n",
    "                    continue\n",
    "\n",
    "                if r > 0: # if not the top row\n",
    "                    Q[r][c][UP] = arb_val\n",
    "\n",
    "                if r < (self.height - 1): # if not the bottom row\n",
    "                    Q[r][c][DOWN] = arb_val\n",
    "\n",
    "                if c > 0: # if not the first col\n",
    "                    Q[r][c][LEFT] = arb_val\n",
    "\n",
    "                if c < (self.width - 1): # if not the first col\n",
    "                    Q[r][c][RIGHT] = arb_val\n",
    "        return Q\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"For printing but mainly for debugging\"\"\"\n",
    "        s = \"\"\n",
    "        for row in range(self.height):\n",
    "            s += \"==\" * (self.width) + \"=\"\n",
    "            s += \"\\n\"\n",
    "            for col in range(self.width):\n",
    "                s += f\"|{self.grid[row][col]}\"\n",
    "            s +=\"|\\n\"\n",
    "        s += \"==\" * (self.width) + \"=\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ba88e",
   "metadata": {},
   "source": [
    "#### Approximating Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q_mc(gridworld, agent, gamma=0.95, num_episodes=500, track=False):\n",
    "    \"\"\"Approximates Q, a gridworld.width by gridworld.height by actions matrix that \n",
    "       contains the average returns from taking each action in each of the possible\n",
    "       grid states across num_episodes \"\"\"\n",
    "    \n",
    "    \"\"\"Initialize Q and returns as matrices that are width * height * actions to store returns\n",
    "       calculated during each episode. For each state/action pair in the returns matrix, a list \n",
    "       of returns for each episode will accumulate. For each state/action pair in the Q matrix,\n",
    "       the average return across episodes will be stored. A list of wallclock_times are stored\n",
    "       to keep track of the time elapsed per episode\"\"\"\n",
    "    Q = [[ {} for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    returns = [[ { } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    episode_returns = []\n",
    "    avg_returns_per_episode = []\n",
    "    wallclock_times = []\n",
    "    \n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        \"\"\"Initialize the time step (t) for the current episode and create lists to store\n",
    "            visited states and taken actions (index matched) \"\"\"\n",
    "        time_step = 0\n",
    "        visited_states_and_taken_actions = list()\n",
    "        \n",
    "        \"\"\"Get a random valid state to place the agent in and select a first action \"\"\"\n",
    "        gridworld.random_start()\n",
    "        selected_action = random.choice(gridworld.agent.available_actions) \n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            \n",
    "            reward_from_action = gridworld.reward(selected_action)\n",
    "\n",
    "            for i in range(len(visited_states_and_taken_actions)):\n",
    "                state_in_history = visited_states_and_taken_actions[-1*i] # moving backwards in time\n",
    "                state_in_history[1] += gamma**i * reward_from_action # element 1 is the cumulative reward\n",
    "                \n",
    "            \"\"\"Store the current state/action pair \"\"\"\n",
    "            visited_states_and_taken_actions.append([(gridworld.agent.state, selected_action), reward_from_action])\n",
    "            \n",
    "            \"\"\"Make the move and increase the time step.\"\"\"\n",
    "            gridworld.move(selected_action)\n",
    "            time_step += 1\n",
    "            \n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            selected_action = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                     [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "        \"\"\"After every episode, add the rewards for each visited state into the returns 3-D array (indexed\n",
    "           by (row, col)). Then recalculate Q based on the ever growing returns lists. As they grow, the\n",
    "           values in Q should converge.\"\"\"\n",
    "        \n",
    "        \"\"\"The return of the episode == The return of the initial (s, a) pair that we chose.\n",
    "        I considered summing all returns from the episode but I don't think that's right.\"\"\"\n",
    "        episode_returns.append(visited_states_and_taken_actions[0][1])\n",
    "        avg_returns_per_episode.append(average(episode_returns))\n",
    "        \n",
    "        for i in range(1, len(visited_states_and_taken_actions)):\n",
    "            step_T_minus_i = visited_states_and_taken_actions[-1*i]\n",
    "            visited_state, taken_action = step_T_minus_i[0]\n",
    "            if (visited_state, taken_action) in map(lambda l : l[0], visited_states_and_taken_actions[0:-1*i]):\n",
    "                continue\n",
    "\n",
    "            rewards = step_T_minus_i[1]\n",
    "            if taken_action not in returns[visited_state.row][visited_state.col]:\n",
    "                returns[visited_state.row][visited_state.col][taken_action] = []\n",
    "            returns[visited_state.row][visited_state.col][taken_action].append(rewards)\n",
    "            Q[visited_state.row][visited_state.col][taken_action] = average(returns[visited_state.row][visited_state.col][taken_action])\n",
    "        \n",
    "        \n",
    "        #avg_returns_per_episode.append(average(returns_per_visited_state))\n",
    "        wallclock_times.append(time.time() - start_time)\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "        \n",
    "    return (Q, avg_returns_per_episode, wallclock_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "010a377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_q_sarsa(gridworld: GridWorld, agent: Agent, gamma=0.95, alpha=0.8, num_episodes=500, track=False):\n",
    "    \n",
    "    #Q = [[ { dir: -0.5 for dir in DIRECTIONS } for _ in range(gridworld.width)] for _ in range(gridworld.height)]\n",
    "    Q = gridworld.init_q()\n",
    "    Q[gridworld.win_state.row][gridworld.win_state.col] = { dir: 0 for dir in DIRECTIONS }\n",
    "\n",
    "    \"\"\"Attach the agent to the gridworld\"\"\"\n",
    "    gridworld.set_agent(agent)\n",
    "    \n",
    "    \"\"\"Loop until all episodes are complete \"\"\"\n",
    "    completed_episodes = 0\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.9\n",
    "    \n",
    "    \"\"\"Get a random valid state to place the agent in and select a first action \"\"\"\n",
    "    gridworld.random_start()\n",
    "    \n",
    "    while completed_episodes < num_episodes:\n",
    "        gridworld.reset_agent()\n",
    "        \n",
    "        \"\"\"Set the next selected action \"\"\"\n",
    "        action_t = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                            [gridworld.agent.state.col], epsilon)\n",
    "        \n",
    "        \"\"\"The agent should act as long as it hasn't reached the terminal state\"\"\"\n",
    "        while gridworld.agent.state != gridworld.win_state:\n",
    "            state_t = gridworld.agent.state\n",
    "\n",
    "            \"\"\"Calculate the reward for the move. Incorporate this reward into the rewards of all states\n",
    "               that have been visited so far this episode.\"\"\"\n",
    "            reward_from_action_t = gridworld.reward(action_t)\n",
    "\n",
    "            \"\"\"Make the move\"\"\"\n",
    "            gridworld.move(action_t)\n",
    "\n",
    "            \"\"\"In case we happen to terminate\"\"\"\n",
    "            if gridworld.agent.state == gridworld.win_state:\n",
    "                break\n",
    "\n",
    "            state_t_plus_one = gridworld.agent.state\n",
    "            \"\"\"Set the next selected action \"\"\"\n",
    "            action_t_plus_one = gridworld.agent.get_action_from_policy(Q[gridworld.agent.state.row]\n",
    "                                                                [gridworld.agent.state.col], epsilon)\n",
    "            \n",
    "            Q[state_t.row][state_t.col][action_t] = Q[state_t.row][state_t.col][action_t] + \\\n",
    "                alpha * (reward_from_action_t + \\\n",
    "                         gamma*Q[state_t_plus_one.row][state_t_plus_one.col][action_t_plus_one] - \\\n",
    "                            Q[state_t.row][state_t.col][action_t])\n",
    "            action_t = action_t_plus_one\n",
    "\n",
    "        epsilon *= 0.999\n",
    "        completed_episodes += 1\n",
    "        \n",
    "        if track and completed_episodes % 100 == 0:\n",
    "            print(f\"Completed episodes: {completed_episodes}\")\n",
    "\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "70a25919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "| | | | | | | | |\n",
      "=================\n",
      "| | | | | | | | |\n",
      "=================\n",
      "| |█| | | | | | |\n",
      "=================\n",
      "| | |█| |*| | | |\n",
      "=================\n",
      "| | | | | |*| | |\n",
      "=================\n",
      "| | | | | | | | |\n",
      "=================\n",
      "| | | | | | |W| |\n",
      "=================\n",
      "| | | | | | | | |\n",
      "=================\n",
      "Completed episodes: 100\n",
      "Completed episodes: 200\n",
      "Completed episodes: 300\n",
      "Completed episodes: 400\n",
      "Completed episodes: 500\n",
      "Completed episodes: 600\n",
      "Completed episodes: 700\n",
      "Completed episodes: 800\n",
      "Completed episodes: 900\n",
      "Completed episodes: 1000\n",
      "Completed episodes: 1100\n",
      "Completed episodes: 1200\n",
      "Completed episodes: 1300\n",
      "Completed episodes: 1400\n",
      "Completed episodes: 1500\n",
      "Completed episodes: 1600\n",
      "Completed episodes: 1700\n",
      "Completed episodes: 1800\n",
      "Completed episodes: 1900\n",
      "Completed episodes: 2000\n",
      "Completed episodes: 2100\n",
      "Completed episodes: 2200\n",
      "Completed episodes: 2300\n",
      "Completed episodes: 2400\n",
      "Completed episodes: 2500\n",
      "Completed episodes: 2600\n",
      "Completed episodes: 2700\n",
      "Completed episodes: 2800\n",
      "Completed episodes: 2900\n",
      "Completed episodes: 3000\n",
      "Completed episodes: 3100\n",
      "Completed episodes: 3200\n",
      "Completed episodes: 3300\n",
      "Completed episodes: 3400\n",
      "Completed episodes: 3500\n",
      "Completed episodes: 3600\n",
      "Completed episodes: 3700\n",
      "Completed episodes: 3800\n",
      "Completed episodes: 3900\n",
      "Completed episodes: 4000\n",
      "Completed episodes: 4100\n",
      "Completed episodes: 4200\n",
      "Completed episodes: 4300\n",
      "Completed episodes: 4400\n",
      "Completed episodes: 4500\n",
      "Completed episodes: 4600\n",
      "Completed episodes: 4700\n",
      "Completed episodes: 4800\n",
      "Completed episodes: 4900\n",
      "Completed episodes: 5000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test approximate_q\"\"\"\n",
    "random.seed(0)\n",
    "height = 8\n",
    "width = 8\n",
    "num_episodes = 5000\n",
    "g = GridWorld(height, width, complex=True)\n",
    "print(g) # print the gridworld\n",
    "\n",
    "# here, returns is an average return value over the entire grid for each episode.\n",
    "# (ex. returns[0] is the avg. return from ep 1, returns[1] is the avg. return from ep 2)\n",
    "#(Q, returns, times) = approximate_q_mc(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)\n",
    "\n",
    "Q = approximate_q_sarsa(g, ArgMaxAgent(), num_episodes=num_episodes, track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7ac0fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "|↓|←|←|→|↓|↓|←|←|\n",
      "=================\n",
      "|↓|←|←|↑|→|↓|→|↑|\n",
      "=================\n",
      "|↓|█|↑|↑|←|→|↓|←|\n",
      "=================\n",
      "|↓|↓|█|↓|█|↑|→|↓|\n",
      "=================\n",
      "|↓|↓|↓|←|↓|█|↓|↓|\n",
      "=================\n",
      "|→|→|→|→|↓|→|↓|←|\n",
      "=================\n",
      "|→|→|↓|→|→|→|→|←|\n",
      "=================\n",
      "|→|→|→|→|→|↑|↑|↑|\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "# plot maximum Q values\n",
    "(q_values, actions) = max_q(Q)\n",
    "plt.imshow(q_values)\n",
    "plt.colorbar()\n",
    "plt.title(\"Q Approximation\")\n",
    "plt.show()\n",
    "\n",
    "print_policy_from_q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301c5b6",
   "metadata": {},
   "source": [
    "### 3 Visualizing Variance-Bias Trade-Off\n",
    "Pick some average return, which constitures roughly the half-way-point between your algorithms average starting return and fully trained return. For both MC- control (from last weeks homework) and 1-step SARSA, do the following: (pick the same state for both!)\n",
    "* For both SARSA and MC-Control:\n",
    "    - Sample 1000 or more episodes starting at some specific (e.g. the starting) state, with some specific action\n",
    "    - Update only this specific starting Q-value!\n",
    "    - Track how the Q-value changes over the episodes (i.e. provide a list or ndarray with an estimation aver each episode)\n",
    "* Repeat the above 100 (or more) times for both SARSA and MC-Control 2\n",
    "* For both SARSA and MC-Control, create a lineplot including mean and std estimation (over the 100+ repeats) vs. episodes sampled\n",
    "* Interpret the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf107e-6e2b-4976-be1d-b0012297dd7c",
   "metadata": {},
   "source": [
    "Notes about Progress\n",
    "----\n",
    "\n",
    "I changed the reward for being in a normal state (not win, warp, or wall) to -0.1 to punish the agent for not being at the win, and now the approximation runs very quickly. It seems to me that the policy must be improved compared to before, because we are not slowing down. \n",
    "\n",
    "ALL OF THE BELOW COMMENTS are before I changed the reward for being in a non win, warp, or wall state to -0.1 instead of 0 .....\n",
    "So far, I added helper function to initialize Q at the beginning of the sarse approximation called 'init_q'. It initializes each valid move to an arbitrary value (-0.5). \n",
    "\n",
    "I also changes the arg_max function to randomly choose if there is more than one action with the same Q value within a state. \n",
    "\n",
    "I also added a randomly chosen start state, instead of (0,0), for literally no reason (feel free to remove).\n",
    "\n",
    "There are some other notes in our notes google doc about some stuff I've been confused about.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
